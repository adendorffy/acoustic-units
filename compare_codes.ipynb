{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "audio_dir = Path(\"librispeech/audio\")\n",
    "audio_ext = \".flac\"\n",
    "paths = list(audio_dir.rglob(f\"**/*{audio_ext}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torchaudio\n",
    "import torch\n",
    "\n",
    "align_dir = Path(\"librispeech/alignments\")\n",
    "align_df = pd.read_csv(align_dir / \"alignments.csv\")\n",
    "\n",
    "model_name = \"hubert_base\"\n",
    "layer = 12\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    bundle = getattr(torchaudio.pipelines, model_name.upper())\n",
    "except AttributeError:\n",
    "    raise ValueError(f\"Invalid model name: {model_name}\")\n",
    "\n",
    "model = bundle.get_model()\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cut_encoding(\n",
    "    encoding: torch.Tensor, word_boundaries: list[float], hop_ms: int = 20\n",
    ") -> torch.Tensor:\n",
    "    hop_size = hop_ms / 1000  # seconds per frame\n",
    "    start_frame = int(word_boundaries[0] / hop_size)\n",
    "    end_frame = int(word_boundaries[1] / hop_size)\n",
    "    return encoding[start_frame:end_frame]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "\n",
    "import numba\n",
    "import numpy as np\n",
    "import scipy.spatial.distance as distance\n",
    "\n",
    "\n",
    "def segment(\n",
    "    sequence: np.ndarray, codebook: np.ndarray, gamma: float\n",
    ") -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"Group speech representations into phone-like segments.\n",
    "\n",
    "    Args:\n",
    "        sequence (NDArray): speech representations of shape (T, D) where T is the number of frames and D is the feature dimension.\n",
    "        codebook (NDArray): cluster centriods of the discrete units of shape (K, D) where K is the number of codes.\n",
    "        gamma float: Duration regularizer weight. Larger values result in a coarser segmentation.\n",
    "\n",
    "    Returns:\n",
    "        NDArray[int]: list of discrete units representing each segment sound types of shape (N,).\n",
    "        NDArray[int]: list of segment boundaries of shape (N+1,).\n",
    "    \"\"\"\n",
    "    dists = distance.cdist(sequence, codebook).astype(np.float32)\n",
    "    alpha, P = _segment(dists, gamma)\n",
    "    return _backtrack(alpha, P)\n",
    "\n",
    "\n",
    "@numba.njit()\n",
    "def _segment(dists, gamma):\n",
    "    T, K = dists.shape\n",
    "\n",
    "    alpha = np.zeros(T + 1, dtype=np.float32)\n",
    "    P = np.zeros((T + 1, 2), dtype=np.int32)\n",
    "    D = np.zeros((T, T, K), dtype=np.float32)\n",
    "\n",
    "    for t in range(T):\n",
    "        for k in range(K):\n",
    "            D[t, t, k] = dists[t, k]\n",
    "    for t in range(T):\n",
    "        for s in range(t + 1, T):\n",
    "            D[t, s, :] = D[t, s - 1, :] + dists[s, :] - gamma\n",
    "\n",
    "    for t in range(T):\n",
    "        alpha[t + 1] = np.inf\n",
    "        for s in range(t + 1):\n",
    "            k = np.argmin(D[s, t, :])\n",
    "            alpha_min = alpha[s] + D[s, t, k]\n",
    "            if alpha_min < alpha[t + 1]:\n",
    "                P[t + 1, :] = s, k\n",
    "                alpha[t + 1] = alpha_min\n",
    "    return alpha, P\n",
    "\n",
    "\n",
    "@numba.njit()\n",
    "def _backtrack(alpha, P):\n",
    "    rhs = len(alpha) - 1\n",
    "    segments = []\n",
    "    boundaries = [rhs]\n",
    "    while rhs != 0:\n",
    "        lhs, code = P[rhs, :]\n",
    "        segments.append(code)\n",
    "        boundaries.append(lhs)\n",
    "        rhs = lhs\n",
    "    segments.reverse()\n",
    "    boundaries.reverse()\n",
    "    return segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "kmeans_path = f\"models/kmeans_{model_name}_layer{layer}_k100.pkl\"\n",
    "kmeans = joblib.load(kmeans_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import itertools\n",
    "import editdistance\n",
    "\n",
    "\n",
    "def normalized_edit_distance(sequences):\n",
    "    pairs = list(itertools.combinations(sequences, 2))\n",
    "    if not pairs:\n",
    "        return 0.0\n",
    "\n",
    "    dists = []\n",
    "    for a, b in pairs:\n",
    "        dist = editdistance.eval(a, b)\n",
    "        norm = dist / max(len(a), len(b))\n",
    "        dists.append(norm)\n",
    "\n",
    "    return np.mean(dists)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 65840 feature files.\n"
     ]
    }
   ],
   "source": [
    "gamma = 1.0\n",
    "feat_paths = list(\n",
    "    Path(f\"my-features/{model_name.upper()}/{layer}/gamma{gamma}\").rglob(\"*.npy\")\n",
    ")\n",
    "print(f\"Found {len(feat_paths)} feature files.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n",
      "1700\n",
      "1800\n",
      "1900\n",
      "2000\n",
      "2100\n",
      "2200\n",
      "2300\n",
      "2400\n",
      "2500\n",
      "2600\n",
      "2700\n",
      "2800\n",
      "2900\n",
      "3000\n",
      "3100\n",
      "3200\n",
      "3300\n",
      "3400\n",
      "3500\n",
      "3600\n",
      "3700\n",
      "3800\n",
      "3900\n",
      "4000\n",
      "4100\n",
      "4200\n",
      "4300\n",
      "4400\n",
      "4500\n",
      "4600\n",
      "4700\n",
      "4800\n",
      "4900\n",
      "5000\n",
      "5100\n",
      "5200\n",
      "5300\n",
      "5400\n",
      "5500\n",
      "5600\n",
      "5700\n",
      "5800\n",
      "5900\n",
      "6000\n",
      "6100\n",
      "6200\n",
      "6300\n",
      "6400\n",
      "6500\n",
      "6600\n",
      "6700\n",
      "6800\n",
      "6900\n",
      "7000\n",
      "7100\n",
      "7200\n",
      "7300\n",
      "7400\n",
      "7500\n",
      "7600\n",
      "7700\n",
      "7800\n",
      "7900\n",
      "8000\n",
      "8100\n",
      "8200\n",
      "8300\n",
      "8400\n",
      "8500\n",
      "8600\n",
      "8700\n",
      "8800\n",
      "8900\n",
      "9000\n",
      "9100\n",
      "9200\n",
      "9300\n",
      "9400\n",
      "9500\n",
      "9600\n",
      "9700\n",
      "9800\n",
      "9900\n",
      "10000\n",
      "10100\n",
      "10200\n",
      "10300\n",
      "10400\n",
      "10500\n",
      "10600\n",
      "10700\n",
      "10800\n",
      "10900\n",
      "11000\n",
      "11100\n",
      "11200\n",
      "11300\n",
      "11400\n",
      "11500\n",
      "11600\n",
      "11700\n",
      "11800\n",
      "11900\n",
      "12000\n",
      "12100\n",
      "12200\n",
      "12300\n",
      "12400\n",
      "12500\n",
      "12600\n",
      "12700\n",
      "12800\n",
      "12900\n",
      "13000\n",
      "13100\n",
      "13200\n",
      "13300\n",
      "13400\n",
      "13500\n",
      "13600\n",
      "13700\n",
      "13800\n",
      "13900\n",
      "14000\n",
      "14100\n",
      "14200\n",
      "14300\n",
      "14400\n",
      "14500\n",
      "14600\n",
      "14700\n",
      "14800\n",
      "14900\n",
      "15000\n",
      "15100\n",
      "15200\n",
      "15300\n",
      "15400\n",
      "15500\n",
      "15600\n",
      "15700\n",
      "15800\n",
      "15900\n",
      "16000\n",
      "16100\n",
      "16200\n",
      "16300\n",
      "16400\n",
      "16500\n",
      "16600\n",
      "16700\n",
      "16800\n",
      "16900\n",
      "17000\n",
      "17100\n",
      "17200\n",
      "17300\n",
      "17400\n",
      "17500\n",
      "17600\n",
      "17700\n",
      "17800\n",
      "17900\n",
      "18000\n",
      "18100\n",
      "18200\n",
      "18300\n",
      "18400\n",
      "18500\n",
      "18600\n",
      "18700\n",
      "18800\n",
      "18900\n",
      "19000\n",
      "19100\n",
      "19200\n",
      "19300\n",
      "19400\n",
      "19500\n",
      "19600\n",
      "19700\n",
      "19800\n",
      "19900\n",
      "20000\n",
      "20100\n",
      "20200\n",
      "20300\n",
      "20400\n",
      "20500\n",
      "20600\n",
      "20700\n",
      "20800\n",
      "20900\n",
      "21000\n",
      "21100\n",
      "21200\n",
      "21300\n",
      "21400\n",
      "21500\n",
      "21600\n",
      "21700\n",
      "21800\n",
      "21900\n",
      "22000\n",
      "22100\n",
      "22200\n",
      "22300\n",
      "22400\n",
      "22500\n",
      "22600\n",
      "22700\n",
      "22800\n",
      "22900\n",
      "23000\n",
      "23100\n",
      "23200\n",
      "23300\n",
      "23400\n",
      "23500\n",
      "23600\n",
      "23700\n",
      "23800\n",
      "23900\n",
      "24000\n",
      "24100\n",
      "24200\n",
      "24300\n",
      "24400\n",
      "24500\n",
      "24600\n",
      "24700\n",
      "24800\n",
      "24900\n",
      "25000\n",
      "25100\n",
      "25200\n",
      "25300\n",
      "25400\n",
      "25500\n",
      "25600\n",
      "25700\n",
      "25800\n",
      "25900\n",
      "26000\n",
      "26100\n",
      "26200\n",
      "26300\n",
      "26400\n",
      "26500\n",
      "26600\n",
      "26700\n",
      "26800\n",
      "26900\n",
      "27000\n",
      "27100\n",
      "27200\n",
      "27300\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[484]\u001b[39m\u001b[32m, line 30\u001b[39m\n\u001b[32m     27\u001b[39m phn_labels.append(phones)\n\u001b[32m     28\u001b[39m word_labels.append(text)\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m codes.append(\u001b[43msegment\u001b[49m\u001b[43m(\u001b[49m\u001b[43mword_encoding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkmeans\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcluster_centers_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1.0\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[32m     32\u001b[39m \u001b[38;5;66;03m# if len(codes) >= 1500:\u001b[39;00m\n\u001b[32m     33\u001b[39m \u001b[38;5;66;03m#     break\u001b[39;00m\n\u001b[32m     35\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(codes) % \u001b[32m100\u001b[39m == \u001b[32m0\u001b[39m:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[349]\u001b[39m\u001b[32m, line 22\u001b[39m, in \u001b[36msegment\u001b[39m\u001b[34m(sequence, codebook, gamma)\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msegment\u001b[39m(\n\u001b[32m      9\u001b[39m     sequence: np.ndarray, codebook: np.ndarray, gamma: \u001b[38;5;28mfloat\u001b[39m\n\u001b[32m     10\u001b[39m ) -> Tuple[np.ndarray, np.ndarray]:\n\u001b[32m     11\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Group speech representations into phone-like segments.\u001b[39;00m\n\u001b[32m     12\u001b[39m \n\u001b[32m     13\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     20\u001b[39m \u001b[33;03m        NDArray[int]: list of segment boundaries of shape (N+1,).\u001b[39;00m\n\u001b[32m     21\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m     dists = \u001b[43mdistance\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcdist\u001b[49m\u001b[43m(\u001b[49m\u001b[43msequence\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcodebook\u001b[49m\u001b[43m)\u001b[49m.astype(np.float32)\n\u001b[32m     23\u001b[39m     alpha, P = _segment(dists, gamma)\n\u001b[32m     24\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _backtrack(alpha, P)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/acoustic-units/.env/lib/python3.12/site-packages/scipy/spatial/distance.py:3127\u001b[39m, in \u001b[36mcdist\u001b[39m\u001b[34m(XA, XB, metric, out, **kwargs)\u001b[39m\n\u001b[32m   3125\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m metric_info \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   3126\u001b[39m     cdist_fn = metric_info.cdist_func\n\u001b[32m-> \u001b[39m\u001b[32m3127\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcdist_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mXA\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mXB\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3128\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m mstr.startswith(\u001b[33m\"\u001b[39m\u001b[33mtest_\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m   3129\u001b[39m     metric_info = _TEST_METRICS.get(mstr, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "codes = []\n",
    "phn_labels = []\n",
    "word_labels = []\n",
    "for path in paths:\n",
    "    wav_df = align_df[align_df[\"filename\"] == path.stem]\n",
    "\n",
    "    waveform, sr_loaded = torchaudio.load(str(path))\n",
    "    if sr_loaded != 16000:\n",
    "        waveform = torchaudio.functional.resample(waveform, sr_loaded, 16000)\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        features, _ = model.extract_features(waveform, num_layers=layer)\n",
    "        encoding = features[layer - 1].squeeze().cpu().numpy()\n",
    "\n",
    "    for w in range(1, max(wav_df[\"word_id\"]) + 1):\n",
    "        word_df = wav_df[wav_df[\"word_id\"] == w]\n",
    "\n",
    "        text = str(word_df[\"text\"].values[0])\n",
    "        phones = tuple(str(word_df[\"phones\"].values[0]).split(\",\"))\n",
    "\n",
    "        start = float(word_df[\"word_start\"].values[0])\n",
    "        end = float(word_df[\"word_end\"].values[0])\n",
    "        word_encoding = cut_encoding(encoding, [start, end])\n",
    "        if len(word_encoding) == 0:\n",
    "            continue\n",
    "        word_codes = kmeans.predict(word_encoding)\n",
    "        phn_labels.append(phones)\n",
    "        word_labels.append(text)\n",
    "\n",
    "        codes.append(segment(word_encoding, kmeans.cluster_centers_, 1.0))\n",
    "\n",
    "        # if len(codes) >= 1500:\n",
    "        #     break\n",
    "\n",
    "        if len(codes) % 100 == 0:\n",
    "            print(f\"{len(codes)}\")\n",
    "\n",
    "    # if len(codes) >= 1500:\n",
    "    #     break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing features: 100%|██████████| 65840/65840 [04:12<00:00, 260.41it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "codes = []\n",
    "phn_labels = []\n",
    "word_labels = []\n",
    "for path in tqdm(feat_paths, desc=\"Processing features\"):\n",
    "    filename = path.stem.split(\"_\")[0]\n",
    "    w = int(path.stem.split(\"_\")[-1])\n",
    "    wav_df = align_df[align_df[\"filename\"] == filename]\n",
    "\n",
    "    word_df = wav_df[wav_df[\"word_id\"] == w]\n",
    "\n",
    "    text = str(word_df[\"text\"].values[0])\n",
    "    phones = tuple(str(word_df[\"phones\"].values[0]).split(\",\"))\n",
    "\n",
    "    phn_labels.append(phones)\n",
    "    word_labels.append(text)\n",
    "\n",
    "    codes.append(np.load(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 543,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K-AH0-N | [73, 29, 35, 43, 93]\n",
      "K-AH0-N | [41, 73, 52, 29, 75, 59]\n",
      "K-AH0-N | [73, 52, 29, 75]\n",
      "K-AE1-N | [41, 73, 42, 4, 70, 99, 53]\n",
      "K-AE1-N | [0, 73, 42, 4, 29, 35, 43]\n",
      "K-AH0-N | [41, 0, 73, 42, 29, 35, 43]\n",
      "K-AH0-N | [0, 73, 42, 29, 35, 93]\n",
      "K-AH0-N | [93, 73, 42, 29, 35]\n",
      "K-AH0-N | [41, 73, 29, 35, 43]\n",
      "K-AH0-N | [73, 29, 35]\n",
      "K-AE1-N | [73, 42, 4, 29, 35, 36]\n",
      "K-AH0-N | [41, 73, 42, 29, 35, 43]\n",
      "K-AH0-N | [41, 0, 73, 52, 29, 35, 43]\n",
      "K-AE1-N | [73, 42, 70, 99]\n",
      "K-AH0-N | [41, 0, 73, 52, 29, 35, 43]\n",
      "K-AH0-N | [73, 29, 35, 43]\n",
      "K-AE1-N | [73, 42, 29, 35, 51]\n",
      "K-AH0-N | [41, 73, 52, 29, 75, 59]\n",
      "K-AH0-N | [73, 42, 29, 75]\n",
      "K-AH0-N | [73, 8]\n",
      "K-AH0-N | [0, 73, 42, 29, 75]\n",
      "K-AH0-N | [41, 73, 29, 35]\n",
      "K-AH0-N | [73, 52, 29, 35, 51]\n",
      "K-AH0-N | [0, 73, 42, 29, 35, 59]\n",
      "K-AH0-N | [41, 73, 29, 35, 18]\n",
      "K-AH0-N | [41, 73, 29, 35]\n",
      "K-AH0-N | [41, 73, 52, 29, 35]\n",
      "K-AH0-N | [73, 29]\n",
      "K-AE1-N | [41, 73, 42, 52, 29, 75, 59]\n",
      "K-AH0-N | [41, 73, 42, 94, 67, 43]\n",
      "K-AE1-N | [93, 73, 42, 29, 75]\n",
      "K-AH0-N | [93, 73, 52, 29, 35, 43, 93]\n",
      "K-AH0-N | [73, 42, 52, 29, 75]\n",
      "K-AH0-N | [73, 42, 75]\n",
      "K-AH0-N | [93, 73, 42, 8, 47, 93]\n",
      "K-AH0-N | [73, 29, 35, 43]\n",
      "K-AH0-N | [41, 73, 29, 75]\n",
      "K-AH0-N | [73, 52, 29, 35, 80]\n",
      "K-AH0-N | [73, 52, 67, 43]\n",
      "K-AE1-N | [0, 73, 42, 4, 75]\n",
      "K-AH0-N | [41, 0, 73, 29, 35, 43]\n",
      "K-AH0-N | [73, 42, 9, 8, 47]\n",
      "K-AH0-N | [41, 73, 29, 75]\n",
      "K-AH0-N | [73, 29, 75]\n",
      "K-AH0-N | [93, 73, 29, 35]\n",
      "K-AH0-N | [73, 29, 43]\n",
      "K-AH0-N | [73, 29, 75, 59]\n",
      "K-AH0-N | [73, 42, 8, 47]\n",
      "K-AH0-N | [73, 29, 59]\n",
      "K-AH0-N | [73, 15, 75]\n",
      "K-AH0-N | [41, 73, 52, 29, 43]\n",
      "K-AH0-N | [73, 29, 35, 43, 47]\n",
      "K-AH0-N | [73, 52, 29]\n",
      "K-AH0-N | [41, 73, 29, 75]\n",
      "K-AH0-N | [73, 30, 8]\n",
      "K-AH0-N | [41, 0, 73, 29, 35]\n",
      "K-AH0-N | [41, 73, 29]\n",
      "K-AE1-N | [0, 73, 42, 77, 4, 75]\n",
      "K-AH0-N | [93, 73, 42, 94, 67, 43]\n",
      "K-AH0-N | [0, 73, 29, 35, 93]\n",
      "K-AH0-N | [41, 73, 52, 8, 47]\n",
      "K-AH0-N | [93, 73, 42, 29, 75]\n",
      "Normalised edit distance (clean):  0.624\n",
      "Normalised edit distance:  0.622\n"
     ]
    }
   ],
   "source": [
    "sil = [\n",
    "    37,\n",
    "    86,\n",
    "    13,\n",
    "    46,\n",
    "    54,\n",
    "    19,\n",
    "    92,\n",
    "    19,\n",
    "    16,\n",
    "    60,\n",
    "    85,\n",
    "    6,\n",
    "    78,\n",
    "    71,\n",
    "    89,\n",
    "    55,\n",
    "    3,\n",
    "    91,\n",
    "    65,\n",
    "    27,\n",
    "    22,\n",
    "    5,\n",
    "]\n",
    "cluster = []\n",
    "dirty_cluster = []\n",
    "for i, c in enumerate(codes):\n",
    "    phones = [phn for phn in phn_labels[i]]\n",
    "\n",
    "    # phones = [phn for phn in phn_labels[i] if phn not in [\"spn\", \"sil\", \"sp\", \"\"]]\n",
    "\n",
    "    if len(c) > 0 and word_labels[i] == \"can\":\n",
    "        clean_codes = [int(c) for c in codes[i] if c not in sil]\n",
    "        cluster.append(clean_codes)\n",
    "        dirty_cluster.append(codes[i])\n",
    "        print(f\"{'-'.join(phones):<5} | {clean_codes}\")\n",
    "\n",
    "print(f\"Normalised edit distance (clean):  {normalized_edit_distance(cluster):.3f}\")\n",
    "print(f\"Normalised edit distance:  {normalized_edit_distance(dirty_cluster):.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 546,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def calc_edit_distance(a, b):\n",
    "    dist = editdistance.eval(a, b)\n",
    "    norm = dist / max(len(a), len(b)) if len(a) > 0 and len(b) > 0 else 1.0\n",
    "\n",
    "    return norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 547,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_codes = codes[0 : int(len(codes) / 4)]\n",
    "subset_phn_labels = phn_labels[0 : int(len(phn_labels) / 4)]\n",
    "subset_word_labels = word_labels[0 : int(len(word_labels) / 4)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating edit distances:   1%|          | 176/16460 [00:05<08:58, 30.23it/s]"
     ]
    }
   ],
   "source": [
    "vals = []\n",
    "rows = []\n",
    "cols = []\n",
    "for i in tqdm(range(len(subset_codes)), desc=\"Calculating edit distances\"):\n",
    "    for j in range(i, len(subset_codes)):\n",
    "        vals.append(calc_edit_distance(subset_codes[i], subset_codes[j]))\n",
    "        rows.append(i)\n",
    "        cols.append(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8242965211138203\n"
     ]
    }
   ],
   "source": [
    "print(np.mean([d for d in vals if d < 1.0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = np.array(rows)\n",
    "cols = np.array(cols)\n",
    "vals = np.array(vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "metadata": {},
   "outputs": [],
   "source": [
    "import igraph as ig\n",
    "\n",
    "g = ig.Graph()\n",
    "g.add_vertices(len(codes))\n",
    "mask = vals < 0.55\n",
    "edges = list(zip(rows[mask], cols[mask]))\n",
    "weights = vals[mask].astype(float)\n",
    "weights = np.where(weights > 0, weights, 1e-10).tolist()\n",
    "\n",
    "if edges:\n",
    "    g.add_edges(edges)\n",
    "    g.es[-len(weights) :].set_attribute_values(\"weight\", weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "metadata": {},
   "outputs": [],
   "source": [
    "import leidenalg as la\n",
    "\n",
    "partition = la.find_partition(\n",
    "    g,\n",
    "    la.CPMVertexPartition,\n",
    "    resolution_parameter=0.13,\n",
    "    weights=\"weight\",\n",
    "    seed=42,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 491 clusters, should be 495\n",
      "20.423% | the, the, the, the, the, the, the, the, the, there, the, the, the, the, the, the, there, they're, the, the, that, the, the, the, the, that, the, the, the, the, than, the, the, the, the, the, them, the, the, the, the, the, the, the, the, the, the, the, the, that, the, the, the, the\n",
      "34.466% | was, was, were, with, was, was, was, which, was, was, was, was, was, with, was, we, was, was, was, was, was, with, was, was, what, would, was, would, what, was, what, was, were, with, was, which, with, way, way\n",
      "36.459% | i, i, i, i, i, or, i, i, an, i, i, i, i, it, i, and, a, on, i, i, it, i, i, i, i, i, i, i, and, i, or\n",
      "26.903% | was, as, was, is, is, was, is, as, is, is, is, as, is, is, has, is, is, is, is, is, is, is, is\n",
      "9.697 % | to, to, to, to, to, to, to, to, to, to, to, to, to, to, to, to, to, to, to, to, to, to\n",
      "40.017% | and, been, an, and, an, in, and, and, in, in, in, in, in, and, in, in, and, on, in\n",
      "10.292% | of, of, of, of, of, of, of, of, of, of, of, of, of, of, of, of, of, of, of\n",
      "49.223% | been, in, than, and, end, and, and, can, been, and, in, rent, end, and, and, and, ann, hand\n",
      "13.403% | be, be, be, dene, be, be, be, be, be, be, be, be, be, by, be, be, be\n",
      "0.000 % | nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, a, nan, nan\n",
      "53.590% | several, several, said, so, some, cellar, say, said, some, so, set, say, such, sorts, somebody\n",
      "25.275% | for, for, for, for, for, for, for, for, for, for, for, free, for, for\n",
      "4.808 % | had, had, had, had, had, had, have, had, have, had, had, had, had, has\n",
      "13.301% | there, the, here, their, there, there, there, there, there, there, their, there, there\n",
      "9.231 % | a, a, a, a, a, a, of, a, a, a, a, a, a\n",
      "18.639% | my, my, might, my, my, might, might, my, my, my, quite, my\n",
      "30.000% | a, a, of, of, of, of, of, of, a, of, of, a\n",
      "22.961% | mean, me, me, me, made, meet, may, me, make, me, me\n",
      "42.701% | and, and, one, in, and, in, and, an, and, and, in\n",
      "5.195 % | no, no, no, know, known, no, no, know, no, no, know\n",
      "30.545% | it, if, it, it, if, at, it, it, it, at, at\n",
      "28.898% | door, more, or, horror, or, door, more, or, or, board\n",
      "48.857% | not, it, at, not, not, know, that, that, that, it\n",
      "8.889 % | that, that, that, that, that, that, that, that, that, that\n",
      "31.270% | by, i, i, my, my, i, my, why, five\n",
      "7.407 % | she, fee, she, she, she, she, she, she, she\n",
      "0.000 % | i, i, i, i, i, i, i, i, i\n",
      "28.237% | wishing, which, which, which, which, which, wishes, which\n",
      "30.918% | it, it, did, it, it, it, out, it\n",
      "34.139% | wet, whatever, whether, wedding, wedding, wedding, wedding, wedding\n",
      "29.252% | like, looked, like, likely, like, look, like\n",
      "28.486% | whole, old, all, all, all, all, all\n",
      "0.000 % | but, but, but, but, but, but\n",
      "13.333% | i, i, i'm, i, i, i\n",
      "45.905% | with, would, when, when, we, when\n",
      "24.000% | chilled, should, should, should, shall, should\n",
      "47.111% | a, it, a, a, her, of\n",
      "38.482% | mention, madge, madge, madge, manage\n",
      "48.333% | he, how, who, it, he\n",
      "29.186% | missus, christmas, missus, else's, missus\n",
      "5.714 % | did, did, did, did, did\n",
      "43.750% | the, a, the, a, that\n",
      "24.000% | in, it, it, in, it\n",
      "41.667% | the, he, the, a, the\n",
      "30.500% | of, of, have, of, of\n",
      "27.679% | what, which, which, but, which\n",
      "0.000 % | mary, mary, mary, mary, mary\n",
      "15.385% | even, given, given, given\n",
      "63.810% | it, you'd, at, that\n",
      "70.000% | a, a, you, to\n",
      "40.693% | mile, might, money, my\n",
      "22.494% | appear, appeared, appeared, upon\n",
      "23.077% | little, lives, little, little\n",
      "7.143 % | one, one, one, what\n",
      "17.725% | light, line, line, lines\n",
      "23.280% | been, bent, been, been\n",
      "18.750% | her, her, heard, her\n",
      "30.000% | to, to, to, to\n",
      "51.978% | provided, crofton, reverberated, profit\n",
      "42.500% | at, up, up, have\n",
      "46.667% | hand, hand, own, ann\n",
      "30.000% | me, me, know, me\n",
      "32.828% | stood, steps, staff, steps\n",
      "46.759% | sprawling, spring, snellings, spend\n",
      "0.000 % | again, again, again, again\n",
      "21.429% | name, name, name, em\n",
      "77.778% | a, it, are, our\n",
      "68.333% | it, who, a, a\n",
      "10.000% | it, it, it, it\n",
      "42.725% | far, five, find, father\n",
      "79.365% | nan, and, or, a\n",
      "24.825% | laughter, porter, water, matter\n",
      "11.111% | last, least, least, least\n",
      "11.111% | give, gives, give, give\n",
      "38.497% | present, persons, pretend, person\n",
      "38.095% | feet, meet, fields\n",
      "55.026% | from, see, free\n",
      "19.048% | bought, what, what\n",
      "30.769% | lighted, lively, lively\n",
      "41.667% | was, hers, hers\n",
      "57.143% | turn, turned, her\n",
      "12.121% | thing, things, thing\n",
      "45.370% | madge, mad, meant\n",
      "66.667% | her, their, are\n",
      "44.444% | she, you, you\n",
      "17.949% | reason, season, cousin\n",
      "20.833% | myself, herself, myself\n",
      "0.000 % | not, not, not\n",
      "0.000 % | nan, nan, nan\n",
      "26.087% | twisted, consisted, twisted\n",
      "0.000 % | a, a, a\n",
      "0.000 % | people, people, people\n",
      "80.000% | at, her, all\n",
      "51.963% | decided, acidity, besides\n",
      "50.712% | sort, some, sunday\n",
      "32.308% | satisfaction, suspicion, supposition\n",
      "9.524 % | journey, journey's, journey\n",
      "0.000 % | ring, ring, ring\n",
      "16.667% | who's, who, who's\n",
      "29.630% | chop, trap, trap\n",
      "25.000% | fashioned, fashioned, fetching\n",
      "48.485% | knocker, knack, nature\n",
      "38.095% | on, owns, on\n",
      "13.333% | that, that's, that\n",
      "38.889% | games, going, going\n",
      "54.545% | i, about, about\n",
      "24.107% | hangar, hundred, hundred\n",
      "45.833% | that, there, they\n",
      "0.000 % | will, will, will\n",
      "18.519% | seemed, seem, seems\n",
      "0.000 % | about, about, about\n",
      "44.444% | a, the, a\n",
      "54.978% | play, pay, pity\n",
      "0.000 % | are, our, our\n",
      "0.000 % | from, from, from\n",
      "18.182% | vast, gasped, gasped\n",
      "43.137% | something, seemed, something\n",
      "0.000 % | it, nan, nan\n",
      "57.143% | were, were, for\n",
      "42.424% | wants, most, wants\n",
      "0.000 % | the, the\n",
      "25.000% | worse, worth\n",
      "30.769% | chorus, course\n",
      "57.143% | door, do\n",
      "22.222% | girl, girls\n",
      "33.333% | advancing, adventuring\n",
      "60.000% | expect, inexplicable\n",
      "0.000 % | hardly, hardly\n",
      "0.000 % | festive, festive\n",
      "80.000% | all, a\n",
      "22.222% | crowd, ploughed\n",
      "14.286% | was, was\n",
      "0.000 % | beyond, beyond\n",
      "0.000 % | nothing, nothing\n",
      "44.444% | kind, can\n",
      "61.538% | table, time\n",
      "44.444% | kept, fact\n",
      "55.556% | miles, most\n",
      "40.000% | in, ann\n",
      "47.368% | feminine, human\n",
      "33.333% | along, long\n",
      "0.000 % | off, off\n",
      "0.000 % | uncommonly, uncommonly\n",
      "0.000 % | voice, voice\n",
      "0.000 % | accident, accident\n",
      "45.455% | maybe, made\n",
      "21.429% | luggage, cottage\n",
      "50.000% | rush, reached\n",
      "62.500% | out, half\n",
      "46.667% | station, stationmaster\n",
      "61.538% | could, witted\n",
      "42.857% | else, us\n",
      "40.000% | hulking, faltering\n",
      "27.273% | better, matter\n",
      "36.364% | hard, cards\n",
      "0.000 % | bell, bell\n",
      "42.857% | horrors, tours\n",
      "28.571% | yet, but\n",
      "40.000% | guineas, business\n",
      "0.000 % | don't, don't\n",
      "20.000% | boy, by\n",
      "0.000 % | that, that\n",
      "75.000% | heard, it\n",
      "85.714% | the, could\n",
      "50.000% | that, out\n",
      "28.571% | well, will\n",
      "0.000 % | except, except\n",
      "0.000 % | expectations, expectations\n",
      "0.000 % | out, out\n",
      "41.667% | three, thirty\n",
      "0.000 % | pounds, pounds\n",
      "27.273% | kinds, signs\n",
      "0.000 % | of, of\n",
      "0.000 % | to, to\n",
      "28.571% | fill, fail\n",
      "33.333% | fact, found\n",
      "0.000 % | notice, notice\n",
      "16.000% | delightfully, delightful\n",
      "28.571% | we, week\n",
      "0.000 % | honeymoon, honeymoon\n",
      "0.000 % | pigstye, pigstye\n",
      "53.846% | pairs, passes\n",
      "28.571% | yes, s\n",
      "21.429% | passage, cottage\n",
      "0.000 % | plain, plain\n",
      "0.000 % | snellings, snellings\n",
      "0.000 % | never, never\n",
      "0.000 % | cousin, cousin\n",
      "0.000 % | ann's, ann's\n",
      "0.000 % | engagement, engagement\n",
      "85.714% | for, but\n",
      "42.857% | not, out\n",
      "0.000 % | macpherson, macpherson\n",
      "0.000 % | you, you\n",
      "62.500% | i, why\n",
      "0.000 % | account, account\n",
      "25.000% | together, altogether\n",
      "63.636% | france, us\n",
      "0.000 % | either, either\n",
      "0.000 % | there's, there's\n",
      "28.571% | getting, going\n",
      "0.000 % | we've\n",
      "0.000 % | mister\n",
      "0.000 % | it\n",
      "0.000 % | impervious\n",
      "0.000 % | of\n",
      "0.000 % | blowing\n",
      "0.000 % | raining\n",
      "0.000 % | trifle\n",
      "0.000 % | odd\n",
      "0.000 % | front\n",
      "0.000 % | wilson\n",
      "0.000 % | illusions\n",
      "0.000 % | presently\n",
      "0.000 % | in\n",
      "0.000 % | a\n",
      "0.000 % | gleamed\n",
      "0.000 % | through\n",
      "0.000 % | window\n",
      "0.000 % | over\n",
      "0.000 % | other\n",
      "0.000 % | after\n",
      "0.000 % | amount\n",
      "0.000 % | unfastening\n",
      "0.000 % | opened\n",
      "0.000 % | the\n",
      "0.000 % | threshold\n",
      "0.000 % | there\n",
      "0.000 % | candle\n",
      "0.000 % | tolled\n",
      "0.000 % | an\n",
      "0.000 % | christmas\n",
      "0.000 % | completely\n",
      "0.000 % | extinct\n",
      "0.000 % | before\n",
      "0.000 % | in\n",
      "0.000 % | quest\n",
      "0.000 % | princely\n",
      "0.000 % | entertainment\n",
      "0.000 % | answer\n",
      "0.000 % | though\n",
      "0.000 % | allowed\n",
      "0.000 % | decent\n",
      "0.000 % | interval\n",
      "0.000 % | imagine\n",
      "0.000 % | christmases\n",
      "0.000 % | chambers\n",
      "0.000 % | it's\n",
      "0.000 % | club\n",
      "0.000 % | cousin\n",
      "0.000 % | lucy's\n",
      "0.000 % | old\n",
      "0.000 % | calls\n",
      "0.000 % | felt\n",
      "0.000 % | quite\n",
      "0.000 % | mingled\n",
      "0.000 % | looking\n",
      "0.000 % | absolutely\n",
      "0.000 % | preposterous\n",
      "0.000 % | ask\n",
      "0.000 % | accommodation\n",
      "0.000 % | nothing\n",
      "0.000 % | about\n",
      "0.000 % | establishment\n",
      "0.000 % | maintained\n",
      "0.000 % | now\n",
      "0.000 % | come\n",
      "0.000 % | village\n",
      "0.000 % | always\n",
      "0.000 % | about\n",
      "0.000 % | half\n",
      "0.000 % | and\n",
      "0.000 % | house\n",
      "0.000 % | move\n",
      "0.000 % | four\n",
      "0.000 % | road\n",
      "0.000 % | across\n",
      "0.000 % | informant\n",
      "0.000 % | mention\n",
      "0.000 % | reply\n",
      "0.000 % | written\n",
      "0.000 % | vague\n",
      "0.000 % | information\n",
      "0.000 % | greeted\n",
      "0.000 % | sounded\n",
      "0.000 % | horrible\n",
      "0.000 % | none\n",
      "0.000 % | em\n",
      "0.000 % | haven't\n",
      "0.000 % | care\n",
      "0.000 % | then\n",
      "0.000 % | night\n",
      "0.000 % | open\n",
      "0.000 % | cart\n",
      "0.000 % | here\n",
      "0.000 % | are\n",
      "0.000 % | retreating\n",
      "0.000 % | feet\n",
      "0.000 % | expostulating\n",
      "0.000 % | darkness\n",
      "0.000 % | silence\n",
      "0.000 % | rousing\n",
      "0.000 % | under\n",
      "0.000 % | such\n",
      "0.000 % | circumstances\n",
      "0.000 % | christian\n",
      "0.000 % | go\n",
      "0.000 % | able\n",
      "0.000 % | reflect\n",
      "0.000 % | warm\n",
      "0.000 % | officials\n",
      "0.000 % | however\n",
      "0.000 % | cold\n",
      "0.000 % | bone\n",
      "0.000 % | tired\n",
      "0.000 % | hungry\n",
      "0.000 % | and\n",
      "0.000 % | who\n",
      "0.000 % | deposited\n",
      "0.000 % | have\n",
      "0.000 % | individual\n",
      "0.000 % | clerk\n",
      "0.000 % | combined\n",
      "0.000 % | lad\n",
      "0.000 % | christopher\n",
      "0.000 % | london\n",
      "0.000 % | suggested\n",
      "0.000 % | driver\n",
      "0.000 % | through\n",
      "0.000 % | empty\n",
      "0.000 % | house\n",
      "0.000 % | remarkable\n",
      "0.000 % | extraordinary\n",
      "0.000 % | predilection\n",
      "0.000 % | notion\n",
      "0.000 % | boyhood\n",
      "0.000 % | upward\n",
      "0.000 % | desired\n",
      "0.000 % | terms\n",
      "0.000 % | length\n",
      "0.000 % | see\n",
      "0.000 % | cover\n",
      "0.000 % | do\n",
      "0.000 % | go\n",
      "0.000 % | lost\n",
      "0.000 % | key\n",
      "0.000 % | knocker\n",
      "0.000 % | think\n",
      "0.000 % | inquired\n",
      "0.000 % | blunderbuss\n",
      "0.000 % | required\n",
      "0.000 % | though\n",
      "0.000 % | than\n",
      "0.000 % | girl\n",
      "0.000 % | what\n",
      "0.000 % | only\n",
      "0.000 % | day\n",
      "0.000 % | just\n",
      "0.000 % | fretting\n",
      "0.000 % | unimportant\n",
      "0.000 % | part\n",
      "0.000 % | am\n",
      "0.000 % | second\n",
      "0.000 % | cook's\n",
      "0.000 % | your\n",
      "0.000 % | i\n",
      "0.000 % | and\n",
      "0.000 % | mamma\n",
      "0.000 % | afford\n",
      "0.000 % | we\n",
      "0.000 % | comfortably\n",
      "0.000 % | italy\n",
      "0.000 % | south\n",
      "0.000 % | turned\n",
      "0.000 % | and\n",
      "0.000 % | year\n",
      "0.000 % | about\n",
      "0.000 % | flow\n",
      "0.000 % | language\n",
      "0.000 % | instance\n",
      "0.000 % | behaviour\n",
      "0.000 % | the\n",
      "0.000 % | having\n",
      "0.000 % | a\n",
      "0.000 % | feeling\n",
      "0.000 % | rancorous\n",
      "0.000 % | woman\n",
      "0.000 % | she\n",
      "0.000 % | they\n",
      "0.000 % | generally\n",
      "0.000 % | have\n",
      "0.000 % | already\n",
      "0.000 % | tendered\n",
      "0.000 % | offerings\n",
      "0.000 % | most\n",
      "0.000 % | her\n",
      "0.000 % | action\n",
      "0.000 % | that\n",
      "0.000 % | springs\n",
      "0.000 % | prodigious\n",
      "0.000 % | cannot\n",
      "0.000 % | explain\n",
      "0.000 % | why\n",
      "0.000 % | romance\n",
      "0.000 % | dead\n",
      "0.000 % | that\n",
      "0.000 % | circle\n",
      "0.000 % | society\n",
      "0.000 % | shirking\n",
      "0.000 % | boys\n",
      "0.000 % | she\n",
      "0.000 % | wrote\n",
      "0.000 % | and\n",
      "0.000 % | suggested\n",
      "0.000 % | has\n",
      "0.000 % | back\n",
      "0.000 % | coupons\n",
      "0.000 % | lodging\n",
      "0.000 % | hotel\n",
      "0.000 % | present\n",
      "0.000 % | of\n",
      "0.000 % | do\n",
      "0.000 % | what\n",
      "0.000 % | her\n",
      "0.000 % | short\n",
      "0.000 % | miraculous\n",
      "0.000 % | to\n",
      "0.000 % | papa\n",
      "0.000 % | remarkable\n",
      "0.000 % | p\n",
      "0.000 % | selfishness\n",
      "0.000 % | positively\n",
      "0.000 % | ours\n",
      "0.000 % | arrangements\n",
      "0.000 % | are\n",
      "0.000 % | the\n",
      "0.000 % | question\n",
      "0.000 % | occurred\n",
      "0.000 % | evening\n",
      "0.000 % | saw\n",
      "0.000 % | possessing\n",
      "0.000 % | relatives\n",
      "0.000 % | do\n",
      "0.000 % | persuaded\n",
      "0.000 % | got\n",
      "0.000 % | thought\n",
      "0.000 % | handed\n",
      "0.000 % | over\n",
      "0.000 % | amount\n",
      "0.000 % | inquired\n",
      "0.000 % | has\n",
      "0.000 % | her\n",
      "0.000 % | love\n",
      "0.000 % | understand\n",
      "0.000 % | bird\n",
      "0.000 % | the\n",
      "0.000 % | bush\n",
      "0.000 % | when\n",
      "0.000 % | it\n",
      "0.000 % | next\n",
      "0.000 % | two\n",
      "0.000 % | be\n",
      "0.000 % | her\n",
      "0.000 % | she\n",
      "11.937%\n"
     ]
    }
   ],
   "source": [
    "print(f\"Found {len(partition)} clusters, should be {len(set(subset_word_labels))}\")\n",
    "dists = []\n",
    "for row in partition:\n",
    "    clust = []\n",
    "    words = []\n",
    "    for el in row:\n",
    "        phones = [\n",
    "            phn for phn in subset_phn_labels[el] if phn not in [\"spn\", \"sil\", \"sp\", \"\"]\n",
    "        ]\n",
    "        word = subset_word_labels[el]\n",
    "        words.append(word)\n",
    "        phones = \"-\".join(phones)\n",
    "\n",
    "        if phones:\n",
    "            clust.append(phones)\n",
    "\n",
    "    if clust:\n",
    "        # print(f\"{','.join([f' {phn}[{wrd}] ' for phn, wrd in zip(clust, words)])}\\n\")\n",
    "\n",
    "        dist = normalized_edit_distance(clust)\n",
    "        dists.append(dist)\n",
    "        print(f\"{dist * 100:<6.3f}% | {', '.join(words)}\")\n",
    "\n",
    "print(f\"{np.mean(dists) * 100:.3f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
