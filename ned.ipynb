{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/danel/.cache/torch/hub/bshall_dusted_main\n",
      "Using cache found in /home/danel/.cache/torch/hub/bshall_dusted_main\n",
      "Using cache found in /home/danel/.cache/torch/hub/bshall_hubert_main\n",
      "Getting units: 100%|██████████| 1/1 [00:00<00:00,  3.83it/s]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from encode import get_units\n",
    "from pathlib import Path\n",
    "\n",
    "align_dir = Path(\"data/alignments/dev-clean/\")\n",
    "align_path = align_dir / \"alignments.csv\"\n",
    "audio_dir = Path(\"data/dev-clean\")\n",
    "audio_ext = \".flac\"\n",
    "\n",
    "align_df = pd.read_csv(align_path)\n",
    "gamma = 0.2\n",
    "layer = 7\n",
    "save_dir = Path(\"ladies/features\")\n",
    "paths = [Path(\"data/dev-clean/174/50561/174-50561-0005.flac\")]\n",
    "get_units(paths, align_df, audio_dir, gamma, layer, save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Appending Features: 100%|██████████| 18/18 [00:00<00:00, 7851.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_samples: 18\n",
      "num_pairs: 153\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing Batches: 100%|██████████| 1/1 [00:00<00:00, 729.19batch/s]\n"
     ]
    }
   ],
   "source": [
    "from dist import get_features, get_batch_of_paths, cal_dist_per_pair\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "out_dir = Path(f\"ladies/output/{gamma}/temp/\")\n",
    "out_dir.mkdir(parents=True, exist_ok=True)\n",
    "preloaded = False\n",
    "chunk_limit = 500000\n",
    "\n",
    "\n",
    "def process_batch(batch, features):\n",
    "    \"\"\"Parallelized function to calculate distance for each (i, j) pair.\"\"\"\n",
    "    return [cal_dist_per_pair(((i, j), (features[i], features[j]))) for i, j in batch]\n",
    "\n",
    "\n",
    "if not preloaded:\n",
    "    paths = (p for p in Path(f\"ladies/features/{gamma}\").rglob(\"**/*.npy\"))\n",
    "    sorted_paths = sorted(paths, key=lambda x: int(x.stem.split(\"_\")[-1]))\n",
    "    sample_size = len(sorted_paths)\n",
    "\n",
    "    features = get_features(sorted_paths)\n",
    "\n",
    "    rows, cols, vals = [], [], []\n",
    "\n",
    "    num_pairs = sample_size * (sample_size - 1) // 2\n",
    "    num_batches = (num_pairs + chunk_limit - 1) // chunk_limit\n",
    "\n",
    "    print(f\"num_samples: {sample_size}\")\n",
    "    print(f\"num_pairs: {num_pairs}\")\n",
    "\n",
    "    chunk_idx = 0\n",
    "    # Parallel execution\n",
    "    for batch in tqdm(\n",
    "        get_batch_of_paths(sample_size, chunk_limit),\n",
    "        total=num_batches,\n",
    "        unit=\"batch\",\n",
    "        mininterval=10.0,\n",
    "        desc=\"Processing Batches\",\n",
    "    ):\n",
    "        for i, j in batch:\n",
    "            i, j, dist = cal_dist_per_pair(((i, j), (features[i], features[j])))\n",
    "            rows.append(i)\n",
    "            cols.append(j)\n",
    "            vals.append(dist)\n",
    "\n",
    "        np.save(out_dir / f\"temp_rows_{chunk_idx}.npy\", rows)\n",
    "        np.save(out_dir / f\"temp_cols_{chunk_idx}.npy\", cols)\n",
    "        np.save(out_dir / f\"temp_vals_{chunk_idx}.npy\", vals)\n",
    "\n",
    "        rows, cols, vals = [], [], []\n",
    "        chunk_idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Calculating total: 100%|██████████| 1/1 [00:00<00:00, 2427.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_size: 153, sample_size: 18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Getting Temp Info: 100%|██████████| 1/1 [00:00<00:00, 995.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph built and saved to ladies/output/0.2/graph.pkl\n",
      "Iteration 1: res=0.020000, Cluster difference=0\n",
      "Best resolution found: 0.020 with cluster difference: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from cluster import build_graph_from_temp, adaptive_res_search\n",
    "import pickle\n",
    "\n",
    "use_preloaded_graph = False\n",
    "num_clusters = 15\n",
    "temp_dir = Path(f\"ladies/output/{gamma}/temp\")\n",
    "temp_dir.mkdir(parents=True, exist_ok=True)  # Ensure the directory exists\n",
    "num_chunks = 1\n",
    "\n",
    "graph_path = Path(f\"ladies/output/{gamma}/graph.pkl\")\n",
    "\n",
    "if use_preloaded_graph and graph_path.exists():\n",
    "    with open(graph_path, \"rb\") as f:\n",
    "        g = pickle.load(f)\n",
    "    print(f\"Loaded precomputed graph from {graph_path}\")\n",
    "else:\n",
    "    g = build_graph_from_temp(temp_dir, num_chunks)\n",
    "    g.write_pickle(str(graph_path))\n",
    "    print(f\"Graph built and saved to {graph_path}\")\n",
    "\n",
    "partition_pattern = Path(f\"ladies/output/{gamma}\").glob(\"partition_r*.csv\")\n",
    "partition_files = list(partition_pattern)\n",
    "\n",
    "if not partition_files:\n",
    "    # No existing partitions found, run the search\n",
    "    best_res, best_partition = adaptive_res_search(g, num_clusters)\n",
    "\n",
    "    # Convert best_partition to a DataFrame\n",
    "    best_partition_df = pd.DataFrame(\n",
    "        {\n",
    "            \"node\": range(len(best_partition.membership)),  # Node IDs\n",
    "            \"cluster\": best_partition.membership,  # Cluster assignments\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Save to CSV\n",
    "    best_partition_df.to_csv(\n",
    "        f\"output/{gamma}/partition_r{round(best_res, 3)}.csv\", index=False\n",
    "    )\n",
    "else:\n",
    "    # Load existing partitions\n",
    "    res_partitions = [\n",
    "        (float(p.stem.split(\"_r\")[1]), pd.read_csv(p)) for p in partition_files\n",
    "    ]\n",
    "\n",
    "    # Find the partition with the minimum resolution\n",
    "    best_res, best_partition_df = min(res_partitions, key=lambda x: x[0])\n",
    "\n",
    "# Ensure best_partition_df is used for further processing\n",
    "actual_clusters = len(set(best_partition_df[\"cluster\"]))\n",
    "diff = abs(actual_clusters - num_clusters)\n",
    "\n",
    "print(f\"Best resolution found: {best_res:.3f} with cluster difference: {diff}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_phones_and_texts(gamma, align_dir):\n",
    "    cache_path = Path(f\"ladies/features/{gamma}/texts_and_phones.csv\")\n",
    "\n",
    "    if cache_path.exists():\n",
    "        df = pd.read_csv(cache_path)\n",
    "        texts = df[\"text\"].tolist()\n",
    "        phones = df[\"phones\"].apply(lambda x: tuple(x.split(\",\")))\n",
    "        print(f\"Loaded texts from {cache_path}\")\n",
    "        return phones, texts\n",
    "\n",
    "    paths = sorted(\n",
    "        Path(f\"ladies/features/{gamma}\").rglob(\"**/*.npy\"),\n",
    "        key=lambda x: int(x.stem.split(\"_\")[-1]),\n",
    "    )\n",
    "    align_df = pd.read_csv(align_dir / \"alignments.csv\")\n",
    "\n",
    "    texts = []\n",
    "    phones = []\n",
    "\n",
    "    for path in tqdm(paths, desc=\"Appending Text and Phones\"):\n",
    "        filename_parts = path.stem.split(\"_\")\n",
    "        wav_df = align_df[align_df[\"filename\"] == filename_parts[0]]\n",
    "        word_df = wav_df[wav_df[\"word_id\"] == int(filename_parts[1])]\n",
    "        texts.append(str(word_df[\"text\"].iloc[0]))\n",
    "        word_phones = [str(word_df[\"phones\"].iloc[0])]\n",
    "        phones.append(tuple(word_phones))\n",
    "\n",
    "    df = pd.DataFrame({\"text\": texts, \"phones\": phones})\n",
    "    df.to_csv(cache_path, index=False)\n",
    "    print(f\"Saved texts to {cache_path}\")\n",
    "\n",
    "    return phones, texts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import editdistance\n",
    "import statistics\n",
    "import itertools\n",
    "\n",
    "\n",
    "def distance(p, q):\n",
    "    \"\"\"Compute normalized edit distance between two strings.\"\"\"\n",
    "    length = max(len(p), len(q))\n",
    "    return (\n",
    "        editdistance.eval(p, q) / length if length > 0 else 1\n",
    "    )  # Avoid division by zero\n",
    "\n",
    "\n",
    "def ned(clusters):\n",
    "    \"\"\"Compute the normalized edit distance (NED) within each cluster.\"\"\"\n",
    "    if not clusters:\n",
    "        return 0\n",
    "\n",
    "    clusters = sorted(clusters, key=lambda x: x[0])\n",
    "\n",
    "    distances = []\n",
    "    for _, group in itertools.groupby(clusters, key=lambda x: x[0]):\n",
    "        group_list = list(group)\n",
    "\n",
    "        if len(group_list) < 2:\n",
    "            continue\n",
    "        print(group_list)\n",
    "\n",
    "        for p, q in itertools.combinations(group_list, 2):\n",
    "            print(p, q)\n",
    "            d = distance(p[1], q[1])\n",
    "            distances.append(d)\n",
    "\n",
    "    return statistics.mean(distances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded texts from ladies/features/0.2/texts_and_phones.csv\n",
      "sil\n",
      "NED: 0\n",
      "\n",
      "L | EY1 | D | IY0\n",
      "NED: 0.9444444444444444\n",
      "\n",
      "sp\n",
      "NED: 0\n",
      "\n",
      "L | EY1 | D | IY0\n",
      "NED: 0.9444444444444444\n",
      "\n",
      "sp\n",
      "NED: 0\n",
      "\n",
      "M | AY1\n",
      "NED: 1.0\n",
      "\n",
      "R | OW1 | Z\n",
      "NED: 1.0\n",
      "\n",
      "W | AY1 | T\n",
      "NED: 1.0\n",
      "\n",
      "L | EY1 | D | IY0\n",
      "NED: 0.9444444444444444\n",
      "\n",
      "sp\n",
      "NED: 0\n",
      "\n",
      "B | AH1 | T\n",
      "NED: 1.0\n",
      "\n",
      "W | IH1 | L\n",
      "NED: 1.0\n",
      "\n",
      "Y | UW1\n",
      "NED: 1.0\n",
      "\n",
      "N | AA1 | T\n",
      "NED: 1.0\n",
      "\n",
      "HH | IY1 | R\n",
      "NED: 1.0\n",
      "\n",
      "AH0\n",
      "NED: 0\n",
      "\n",
      "R | AW1 | N | D | AH0 | L\n",
      "NED: 0.9777777777777777\n",
      "\n",
      "L | EY1 | D | IY0\n",
      "NED: 0.9444444444444444\n",
      "\n"
     ]
    }
   ],
   "source": [
    "phones, texts = get_phones_and_texts(gamma, align_dir)\n",
    "\n",
    "\n",
    "def ned_per_cluster(cluster):\n",
    "    distances = []\n",
    "\n",
    "    if len(cluster) < 2:\n",
    "        return 0\n",
    "\n",
    "    for p, q in itertools.combinations(cluster, 2):\n",
    "        if p not in {\"sil\", \"sp\"} and q not in {\"sil\", \"sp\"}:\n",
    "            d = distance(p, q)\n",
    "            distances.append(d)\n",
    "\n",
    "    return statistics.mean(distances) if distances else 0\n",
    "\n",
    "\n",
    "def clean_phones(phones):\n",
    "    phone_clusters = []\n",
    "\n",
    "    for node_id, phone in phones.items():\n",
    "        clean_phone = []\n",
    "        for el in phone:\n",
    "            if el.strip() in {\"(\", \")\"}:\n",
    "                continue\n",
    "            clean_el = el.strip(\"'()'\")\n",
    "            clean_phone.append(clean_el)\n",
    "        phone_clusters.append((node_id, tuple(clean_phone)))\n",
    "    return phone_clusters\n",
    "\n",
    "\n",
    "phones = clean_phones(phones)\n",
    "phone_clusters = []\n",
    "for node, cluster in zip(best_partition_df[\"node\"], best_partition_df[\"cluster\"]):\n",
    "    for node_id, phone in phones:\n",
    "        if node_id == node:\n",
    "            phone_clusters.append((cluster, phone))\n",
    "\n",
    "# text_clusters = transcribe_clusters(best_partition_df, texts)  # if you want to print it\n",
    "for cluster_id, phones in phone_clusters:\n",
    "    print(\" | \".join(phones))\n",
    "    print(f\"NED: {ned_per_cluster(phones)}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "def print_clusters(cluster_words, cluster_phones):\n",
    "    # Dictionary to store all text per cluster\n",
    "    cluster_texts = defaultdict(list)\n",
    "\n",
    "    # Group all text by cluster_id\n",
    "    for cluster_id, txt in cluster_words:\n",
    "        cluster_texts[cluster_id].append(txt)\n",
    "\n",
    "    cluster_phones = sorted(cluster_phones, key=lambda x: x[0])\n",
    "    grouped_cluster_phones = []\n",
    "    for _, group in itertools.groupby(cluster_phones, key=lambda x: x[0]):\n",
    "        grouped_cluster_phones.append(list(group))\n",
    "\n",
    "    # Print all texts in each cluster\n",
    "    neds = []\n",
    "    for cluster_id, texts in cluster_texts.items():\n",
    "        if len(texts) > 1:\n",
    "            phones = grouped_cluster_phones[cluster_id]\n",
    "            ned_val = ned_per_cluster(phones)\n",
    "            neds.append(ned_val)\n",
    "            words = []\n",
    "            for text in texts:\n",
    "                text = str(text)\n",
    "                if text != \"nan\":\n",
    "                    words.append(text)\n",
    "            if words:\n",
    "                print(f\"Cluster {cluster_id}: ned: {ned_val}\\n {' | '.join(words)}\\n\")\n",
    "    print(f\"NED: {statistics.mean(neds)}\")\n",
    "\n",
    "\n",
    "print_clusters(text_clusters, phone_clusters)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
