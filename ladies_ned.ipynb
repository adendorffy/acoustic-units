{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from encode import get_units\n",
    "from pathlib import Path\n",
    "\n",
    "get = False\n",
    "\n",
    "align_dir = Path(\"data/alignments/dev-clean/\")\n",
    "align_path = align_dir / \"alignments.csv\"\n",
    "audio_dir = Path(\"data/dev-clean\")\n",
    "audio_ext = \".flac\"\n",
    "\n",
    "align_df = pd.read_csv(align_path)\n",
    "gamma = 0.2\n",
    "layer = 7\n",
    "save_dir = Path(\"ladies/features\")\n",
    "paths = [Path(\"data/dev-clean/174/50561/174-50561-0005.flac\")]\n",
    "\n",
    "if get:\n",
    "    get_units(paths, align_df, audio_dir, gamma, layer, save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Appending Features: 100%|██████████| 18/18 [00:00<00:00, 10357.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_samples: 18\n",
      "num_pairs: 153\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches: 100%|██████████| 1/1 [00:00<00:00, 936.02batch/s]\n"
     ]
    }
   ],
   "source": [
    "from dist import get_features, get_batch_of_paths, cal_dist_per_pair\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "out_dir = Path(f\"ladies/output/{gamma}/temp/\")\n",
    "out_dir.mkdir(parents=True, exist_ok=True)\n",
    "preloaded = False\n",
    "chunk_limit = 500000\n",
    "\n",
    "\n",
    "def process_batch(batch, features):\n",
    "    \"\"\"Parallelized function to calculate distance for each (i, j) pair.\"\"\"\n",
    "    return [cal_dist_per_pair(((i, j), (features[i], features[j]))) for i, j in batch]\n",
    "\n",
    "\n",
    "if not preloaded:\n",
    "    paths = (p for p in Path(f\"ladies/features/{gamma}\").rglob(\"**/*.npy\"))\n",
    "    sorted_paths = sorted(paths, key=lambda x: int(x.stem.split(\"_\")[-1]))\n",
    "    sample_size = len(sorted_paths)\n",
    "\n",
    "    features = get_features(sorted_paths)\n",
    "\n",
    "    rows, cols, vals = [], [], []\n",
    "\n",
    "    num_pairs = sample_size * (sample_size - 1) // 2\n",
    "    num_batches = (num_pairs + chunk_limit - 1) // chunk_limit\n",
    "\n",
    "    print(f\"num_samples: {sample_size}\")\n",
    "    print(f\"num_pairs: {num_pairs}\")\n",
    "\n",
    "    chunk_idx = 0\n",
    "    # Parallel execution\n",
    "    for batch in tqdm(\n",
    "        get_batch_of_paths(sample_size, chunk_limit),\n",
    "        total=num_batches,\n",
    "        unit=\"batch\",\n",
    "        mininterval=10.0,\n",
    "        desc=\"Processing Batches\",\n",
    "    ):\n",
    "        for i, j in batch:\n",
    "            i, j, dist = cal_dist_per_pair(((i, j), (features[i], features[j])))\n",
    "            rows.append(i)\n",
    "            cols.append(j)\n",
    "            vals.append(dist)\n",
    "\n",
    "        np.save(out_dir / f\"temp_rows_{chunk_idx}.npy\", rows)\n",
    "        np.save(out_dir / f\"temp_cols_{chunk_idx}.npy\", cols)\n",
    "        np.save(out_dir / f\"temp_vals_{chunk_idx}.npy\", vals)\n",
    "\n",
    "        rows, cols, vals = [], [], []\n",
    "        chunk_idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating total: 100%|██████████| 1/1 [00:00<00:00, 4084.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_size: 153, sample_size: 18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Getting Temp Info: 100%|██████████| 1/1 [00:00<00:00, 1312.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph built and saved to ladies/output/0.2/graph.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from cluster import build_graph_from_temp\n",
    "import pickle\n",
    "\n",
    "use_preloaded_graph = False\n",
    "temp_dir = Path(f\"ladies/output/{gamma}/temp\")\n",
    "temp_dir.mkdir(parents=True, exist_ok=True)  # Ensure the directory exists\n",
    "num_chunks = 1\n",
    "\n",
    "graph_path = Path(f\"ladies/output/{gamma}/graph.pkl\")\n",
    "\n",
    "if use_preloaded_graph and graph_path.exists():\n",
    "    with open(graph_path, \"rb\") as f:\n",
    "        g = pickle.load(f)\n",
    "    print(f\"Loaded precomputed graph from {graph_path}\")\n",
    "else:\n",
    "    g = build_graph_from_temp(temp_dir, num_chunks, threshold=0.7)\n",
    "    g.write_pickle(str(graph_path))\n",
    "    print(f\"Graph built and saved to {graph_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1: res=0.210000, Cluster difference=3\n",
      "Iteration 2: res=0.209000, Cluster difference=3\n",
      "Iteration 3: res=0.208145, Cluster difference=3\n",
      "Iteration 4: res=0.207414, Cluster difference=3\n",
      "Iteration 5: res=0.206789, Cluster difference=3\n",
      "Iteration 6: res=0.206255, Cluster difference=3\n",
      "Iteration 7: res=0.205798, Cluster difference=3\n",
      "Iteration 8: res=0.205407, Cluster difference=3\n",
      "Iteration 9: res=0.205073, Cluster difference=3\n",
      "Iteration 10: res=0.204787, Cluster difference=3\n",
      "Iteration 11: res=0.204543, Cluster difference=3\n",
      "Iteration 12: res=0.204334, Cluster difference=3\n",
      "Iteration 13: res=0.204156, Cluster difference=3\n",
      "Iteration 14: res=0.204003, Cluster difference=3\n",
      "Iteration 15: res=0.203873, Cluster difference=3\n",
      "Iteration 16: res=0.203761, Cluster difference=3\n",
      "Iteration 17: res=0.203666, Cluster difference=3\n",
      "Iteration 18: res=0.203584, Cluster difference=3\n",
      "Iteration 19: res=0.203515, Cluster difference=3\n",
      "Iteration 20: res=0.203455, Cluster difference=3\n",
      "Iteration 21: res=0.203404, Cluster difference=3\n",
      "Iteration 22: res=0.203360, Cluster difference=3\n",
      "Iteration 23: res=0.203323, Cluster difference=3\n",
      "Iteration 24: res=0.203291, Cluster difference=3\n",
      "Iteration 25: res=0.203264, Cluster difference=3\n",
      "Iteration 26: res=0.203241, Cluster difference=3\n",
      "Iteration 27: res=0.203221, Cluster difference=3\n",
      "Iteration 28: res=0.203204, Cluster difference=3\n",
      "Iteration 29: res=0.203189, Cluster difference=3\n",
      "Iteration 30: res=0.203177, Cluster difference=3\n",
      "Iteration 31: res=0.203166, Cluster difference=3\n",
      "Iteration 32: res=0.203157, Cluster difference=3\n",
      "Iteration 33: res=0.203149, Cluster difference=3\n",
      "Iteration 34: res=0.203143, Cluster difference=3\n",
      "Iteration 35: res=0.203137, Cluster difference=3\n",
      "Iteration 36: res=0.203132, Cluster difference=3\n",
      "Iteration 37: res=0.203128, Cluster difference=3\n",
      "Iteration 38: res=0.203124, Cluster difference=3\n",
      "Iteration 39: res=0.203121, Cluster difference=3\n",
      "Iteration 40: res=0.203119, Cluster difference=3\n",
      "Iteration 41: res=0.203117, Cluster difference=3\n",
      "Iteration 42: res=0.203115, Cluster difference=3\n",
      "Iteration 43: res=0.203113, Cluster difference=3\n",
      "Iteration 44: res=0.203112, Cluster difference=3\n",
      "Iteration 45: res=0.203110, Cluster difference=3\n",
      "Iteration 46: res=0.203109, Cluster difference=3\n",
      "Res is stabilising. Abort.\n"
     ]
    }
   ],
   "source": [
    "from cluster import adaptive_res_search\n",
    "\n",
    "num_clusters = 18\n",
    "best_res, best_partition = adaptive_res_search(g, num_clusters)\n",
    "\n",
    "# Convert best_partition to a DataFrame\n",
    "best_partition_df = pd.DataFrame(\n",
    "    {\n",
    "        \"node\": range(len(best_partition.membership)),  # Node IDs\n",
    "        \"cluster\": best_partition.membership,  # Cluster assignments\n",
    "    }\n",
    ")\n",
    "\n",
    "# Save to CSV\n",
    "best_partition_df.to_csv(\n",
    "    f\"output/{gamma}/partition_r{round(best_res, 3)}.csv\", index=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best resolution found: 0.210 with cluster difference: 3\n"
     ]
    }
   ],
   "source": [
    "actual_clusters = len(set(best_partition_df[\"cluster\"]))\n",
    "diff = abs(actual_clusters - num_clusters)\n",
    "\n",
    "print(f\"Best resolution found: {best_res:.3f} with cluster difference: {diff}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded texts from ladies/features/0.2/texts_and_phones.csv\n"
     ]
    }
   ],
   "source": [
    "def get_phones_and_texts(gamma, align_dir):\n",
    "    cache_path = Path(f\"ladies/features/{gamma}/texts_and_phones.csv\")\n",
    "\n",
    "    if cache_path.exists():\n",
    "        df = pd.read_csv(cache_path)\n",
    "        texts = df[\"text\"].tolist()\n",
    "        phones = df[\"phones\"].apply(lambda x: tuple(x.split(\",\")))\n",
    "        print(f\"Loaded texts from {cache_path}\")\n",
    "        return phones, texts\n",
    "\n",
    "    paths = sorted(\n",
    "        Path(f\"ladies/features/{gamma}\").rglob(\"**/*.npy\"),\n",
    "        key=lambda x: int(x.stem.split(\"_\")[-1]),\n",
    "    )\n",
    "    align_df = pd.read_csv(align_dir / \"alignments.csv\")\n",
    "\n",
    "    texts = []\n",
    "    phones = []\n",
    "\n",
    "    for path in tqdm(paths, desc=\"Appending Text and Phones\"):\n",
    "        filename_parts = path.stem.split(\"_\")\n",
    "        wav_df = align_df[align_df[\"filename\"] == filename_parts[0]]\n",
    "        word_df = wav_df[wav_df[\"word_id\"] == int(filename_parts[1])]\n",
    "        texts.append(str(word_df[\"text\"].iloc[0]))\n",
    "        word_phones = word_df[\"phones\"].iloc[0].split(\",\")\n",
    "        word_phones = \" \".join(word_phones)\n",
    "        phones.append(word_phones)\n",
    "\n",
    "    df = pd.DataFrame({\"text\": texts, \"phones\": phones})\n",
    "    df.to_csv(cache_path, index=False)\n",
    "    print(f\"Saved texts to {cache_path}\")\n",
    "\n",
    "    return phones, texts\n",
    "\n",
    "\n",
    "phones, texts = get_phones_and_texts(gamma, align_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0                 (sil,)\n",
      "1         (L EY1 D IY0,)\n",
      "2                  (sp,)\n",
      "3         (L EY1 D IY0,)\n",
      "4                  (sp,)\n",
      "5               (M AY1,)\n",
      "6             (R OW1 Z,)\n",
      "7             (W AY1 T,)\n",
      "8         (L EY1 D IY0,)\n",
      "9                  (sp,)\n",
      "10            (B AH1 T,)\n",
      "11            (W IH1 L,)\n",
      "12              (Y UW1,)\n",
      "13            (N AA1 T,)\n",
      "14           (HH IY1 R,)\n",
      "15                (AH0,)\n",
      "16    (R AW1 N D AH0 L,)\n",
      "17        (L EY1 D IY0,)\n",
      "Name: phones, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(phones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuple_phones = []\n",
    "for id, word_phones in enumerate(phones):\n",
    "    word_phones_tuple = tuple(word_phones[0].split(\" \"))\n",
    "    text = texts[id]\n",
    "    tuple_phones.append((id, word_phones_tuple, text))\n",
    "\n",
    "del phones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, ('sil',), nan), (1, ('L', 'EY1', 'D', 'IY0'), 'lady'), (2, ('sp',), nan), (3, ('L', 'EY1', 'D', 'IY0'), 'lady'), (4, ('sp',), nan), (5, ('M', 'AY1'), 'my'), (6, ('R', 'OW1', 'Z'), 'rose'), (7, ('W', 'AY1', 'T'), 'white'), (8, ('L', 'EY1', 'D', 'IY0'), 'lady'), (9, ('sp',), nan), (10, ('B', 'AH1', 'T'), 'but'), (11, ('W', 'IH1', 'L'), 'will'), (12, ('Y', 'UW1'), 'you'), (13, ('N', 'AA1', 'T'), 'not'), (14, ('HH', 'IY1', 'R'), 'hear'), (15, ('AH0',), 'a'), (16, ('R', 'AW1', 'N', 'D', 'AH0', 'L'), 'roundel'), (17, ('L', 'EY1', 'D', 'IY0'), 'lady')]\n"
     ]
    }
   ],
   "source": [
    "print(tuple_phones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_tuples = []\n",
    "seen_nodes = set()  # To track nodes we've already added\n",
    "\n",
    "for node_id, cluster in zip(best_partition_df[\"node\"], best_partition_df[\"cluster\"]):\n",
    "    for node, phone, word in tuple_phones:\n",
    "        if node_id == node and node_id not in seen_nodes:\n",
    "            cluster_tuples.append((cluster, phone, word))\n",
    "            seen_nodes.add(node_id)  # Mark this node as added\n",
    "            break  # Exit loop early once node is matched\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "idx: 0\n",
      "L-EY-D-IY, L-EY-D-IY: dist 0.0\n",
      "\n",
      "idx: 1\n",
      "W-IH-L, R-AW-N-D-AH-L: dist 0.8333333333333334\n",
      "\n",
      "idx: 2\n",
      "L-EY-D-IY, L-EY-D-IY: dist 0.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "import editdistance\n",
    "import re\n",
    "\n",
    "sorted_clusters = sorted(cluster_tuples, key=lambda x: x[0])\n",
    "distances = []\n",
    "for idx, group in itertools.groupby(sorted_clusters, key=lambda x: x[0]):\n",
    "    group_list = list(group)\n",
    "\n",
    "    if len(group_list) < 2:\n",
    "        continue\n",
    "\n",
    "    print(f\"idx: {idx}\")\n",
    "    for p, q in itertools.combinations(group_list, 2):\n",
    "        p_1 = tuple(re.sub(r\"[012]\", \"\", phn) for phn in p[1] if phn != \"sil\")\n",
    "        q_1 = tuple(re.sub(r\"[012]\", \"\", phn) for phn in q[1] if phn != \"sil\")\n",
    "\n",
    "        d = 1.0\n",
    "        if max(len(p_1), len(q_1)) > 0:\n",
    "            d = float(editdistance.eval(p_1, q_1)) / max(len(p_1), len(q_1))\n",
    "\n",
    "        print(f\"{'-'.join(p_1)}, {'-'.join(q_1)}: dist {d}\")\n",
    "        distances.append(d)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NED: 0.2777777777777778\n"
     ]
    }
   ],
   "source": [
    "import statistics\n",
    "\n",
    "print(f\"NED: {statistics.mean(distances)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
