{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from encode import sample_files, get_units\n",
    "from distance import get_batch_of_paths, calculate_distance_per_chunk_pair\n",
    "from tqdm import tqdm\n",
    "import scipy.sparse as sp\n",
    "import editdistance\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_dir = Path(\"data/dev-clean\")\n",
    "audio_ext = \".flac\"\n",
    "align_path = Path(\"data/alignments/dev-clean/alignments.csv\")\n",
    "save_dir = Path(\"features/\")\n",
    "wav_dir = Path(\"data/dev-clean\")\n",
    "feat_dir = Path(\"features/0.2\")\n",
    "\n",
    "align_df = pd.read_csv(align_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "# feature_paths, sample_size = sample_files(feature_dir=feat_dir, sample_size=100)\n",
    "paths = [Path(\"data/dev-clean/174/50561/174-50561-0005.flac\")]\n",
    "sample_size = len(paths)\n",
    "print(sample_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/danel/.cache/torch/hub/bshall_dusted_main\n",
      "Using cache found in /home/danel/.cache/torch/hub/bshall_dusted_main\n",
      "Using cache found in /home/danel/.cache/torch/hub/bshall_hubert_main\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "from webrtcvad import Vad\n",
    "from encode import mark_sil\n",
    "\n",
    "kmeans, segment = torch.hub.load(\n",
    "    \"bshall/dusted:main\", \"kmeans\", language=\"english\", trust_repo=True\n",
    ")\n",
    "hubert, encode = torch.hub.load(\n",
    "    \"bshall/dusted:main\", \"hubert\", language=\"english\", trust_repo=True\n",
    ")\n",
    "vad = Vad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "path: data/dev-clean/174/50561/174-50561-0005.flac\n",
      "encoding_shape: torch.Size([285, 768])\n",
      "flags_shape: 285\n",
      "word: nan\n",
      "word_boundaries: [np.float64(0.0), np.float64(0.51)]\n",
      "frames: (0, 25)\n",
      "cut_encoding_shape: torch.Size([25, 768])\n",
      "cut_flags_shape: 25\n",
      "cut_flags: [False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False]\n",
      "clean_encoding_shape: 0\n",
      "codes: []\n",
      "\n",
      "word: lady\n",
      "word_boundaries: [np.float64(0.51), np.float64(1.06)]\n",
      "frames: (25, 53)\n",
      "cut_encoding_shape: torch.Size([28, 768])\n",
      "cut_flags_shape: 28\n",
      "cut_flags: [False, False, False, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True]\n",
      "clean_encoding_shape: 25\n",
      "codes: [17 95 48 20 82 40 57 91 16]\n",
      "\n",
      "word: nan\n",
      "word_boundaries: [np.float64(1.06), np.float64(1.12)]\n",
      "frames: (53, 56)\n",
      "cut_encoding_shape: torch.Size([3, 768])\n",
      "cut_flags_shape: 3\n",
      "cut_flags: [True, True, True]\n",
      "clean_encoding_shape: 3\n",
      "codes: [16]\n",
      "\n",
      "word: lady\n",
      "word_boundaries: [np.float64(1.12), np.float64(1.7)]\n",
      "frames: (56, 85)\n",
      "cut_encoding_shape: torch.Size([29, 768])\n",
      "cut_flags_shape: 29\n",
      "cut_flags: [True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True]\n",
      "clean_encoding_shape: 29\n",
      "codes: [16 17 95 48 20 82 40 57 91  5 16]\n",
      "\n",
      "word: nan\n",
      "word_boundaries: [np.float64(1.7), np.float64(1.73)]\n",
      "frames: (85, 86)\n",
      "cut_encoding_shape: torch.Size([1, 768])\n",
      "cut_flags_shape: 1\n",
      "cut_flags: [True]\n",
      "clean_encoding_shape: 1\n",
      "codes: [16]\n",
      "\n",
      "word: my\n",
      "word_boundaries: [np.float64(1.73), np.float64(1.89)]\n",
      "frames: (86, 94)\n",
      "cut_encoding_shape: torch.Size([8, 768])\n",
      "cut_flags_shape: 8\n",
      "cut_flags: [True, True, True, True, True, True, True, True]\n",
      "clean_encoding_shape: 8\n",
      "codes: [16 90 25 43 70 51]\n",
      "\n",
      "word: rose\n",
      "word_boundaries: [np.float64(1.89), np.float64(2.26)]\n",
      "frames: (94, 113)\n",
      "cut_encoding_shape: torch.Size([19, 768])\n",
      "cut_flags_shape: 19\n",
      "cut_flags: [True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True]\n",
      "clean_encoding_shape: 19\n",
      "codes: [20 13  8 85 75 73 92 30 59]\n",
      "\n",
      "word: white\n",
      "word_boundaries: [np.float64(2.26), np.float64(2.51)]\n",
      "frames: (113, 125)\n",
      "cut_encoding_shape: torch.Size([12, 768])\n",
      "cut_flags_shape: 12\n",
      "cut_flags: [True, True, True, True, True, True, True, True, True, True, True, True]\n",
      "clean_encoding_shape: 12\n",
      "codes: [13 43 51 20 68]\n",
      "\n",
      "word: lady\n",
      "word_boundaries: [np.float64(2.51), np.float64(2.99)]\n",
      "frames: (125, 149)\n",
      "cut_encoding_shape: torch.Size([24, 768])\n",
      "cut_flags_shape: 24\n",
      "cut_flags: [True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True]\n",
      "clean_encoding_shape: 24\n",
      "codes: [17 95 48 20 82 40 91  5 32]\n",
      "\n",
      "word: nan\n",
      "word_boundaries: [np.float64(2.99), np.float64(3.44)]\n",
      "frames: (149, 172)\n",
      "cut_encoding_shape: torch.Size([23, 768])\n",
      "cut_flags_shape: 23\n",
      "cut_flags: [True, True, True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False]\n",
      "clean_encoding_shape: 3\n",
      "codes: [32 74 45]\n",
      "\n",
      "word: but\n",
      "word_boundaries: [np.float64(3.44), np.float64(3.65)]\n",
      "frames: (172, 182)\n",
      "cut_encoding_shape: torch.Size([10, 768])\n",
      "cut_flags_shape: 10\n",
      "cut_flags: [False, False, False, True, True, True, True, True, True, True]\n",
      "clean_encoding_shape: 7\n",
      "codes: [94 85 68 13]\n",
      "\n",
      "word: will\n",
      "word_boundaries: [np.float64(3.65), np.float64(3.83)]\n",
      "frames: (182, 191)\n",
      "cut_encoding_shape: torch.Size([9, 768])\n",
      "cut_flags_shape: 9\n",
      "cut_flags: [True, True, True, True, True, True, True, True, True]\n",
      "clean_encoding_shape: 9\n",
      "codes: [13 84 77 41]\n",
      "\n",
      "word: you\n",
      "word_boundaries: [np.float64(3.83), np.float64(3.96)]\n",
      "frames: (191, 198)\n",
      "cut_encoding_shape: torch.Size([7, 768])\n",
      "cut_flags_shape: 7\n",
      "cut_flags: [True, True, True, True, True, True, True]\n",
      "clean_encoding_shape: 7\n",
      "codes: [40 13 28 62]\n",
      "\n",
      "word: not\n",
      "word_boundaries: [np.float64(3.96), np.float64(4.12)]\n",
      "frames: (198, 206)\n",
      "cut_encoding_shape: torch.Size([8, 768])\n",
      "cut_flags_shape: 8\n",
      "cut_flags: [True, True, True, True, True, True, True, True]\n",
      "clean_encoding_shape: 8\n",
      "codes: [ 1 79 43 70 27 82]\n",
      "\n",
      "word: hear\n",
      "word_boundaries: [np.float64(4.12), np.float64(4.45)]\n",
      "frames: (206, 222)\n",
      "cut_encoding_shape: torch.Size([16, 768])\n",
      "cut_flags_shape: 16\n",
      "cut_flags: [True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True]\n",
      "clean_encoding_shape: 16\n",
      "codes: [65 28 86 10]\n",
      "\n",
      "word: a\n",
      "word_boundaries: [np.float64(4.45), np.float64(4.56)]\n",
      "frames: (222, 228)\n",
      "cut_encoding_shape: torch.Size([6, 768])\n",
      "cut_flags_shape: 6\n",
      "cut_flags: [True, True, True, True, True, True]\n",
      "clean_encoding_shape: 6\n",
      "codes: [31 66 18 24]\n",
      "\n",
      "word: roundel\n",
      "word_boundaries: [np.float64(4.56), np.float64(4.99)]\n",
      "frames: (228, 249)\n",
      "cut_encoding_shape: torch.Size([21, 768])\n",
      "cut_flags_shape: 21\n",
      "cut_flags: [True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True]\n",
      "clean_encoding_shape: 21\n",
      "codes: [13  8 43 71 52 26 55 77 41]\n",
      "\n",
      "word: lady\n",
      "word_boundaries: [np.float64(4.99), np.float64(5.49)]\n",
      "frames: (249, 274)\n",
      "cut_encoding_shape: torch.Size([25, 768])\n",
      "cut_flags_shape: 25\n",
      "cut_flags: [True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True]\n",
      "clean_encoding_shape: 25\n",
      "codes: [17 95 48 20 82 40 91  5 32]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def get_frame_num(timestamp: float, sample_rate: int, frame_size_ms: int) -> int:\n",
    "    hop = frame_size_ms / 1000 * sample_rate\n",
    "    hop_size = np.max([hop, 1])\n",
    "    return int((timestamp * sample_rate) / hop_size)\n",
    "\n",
    "\n",
    "for path in paths:\n",
    "    print(f\"path: {path}\")\n",
    "    wav_df = align_df[align_df[\"filename\"] == path.stem]\n",
    "\n",
    "    wav, sr = torchaudio.load(str(path))\n",
    "    wav = torchaudio.functional.resample(wav, sr, 16000)\n",
    "    flags = mark_sil(vad, wav)\n",
    "    wav = wav.unsqueeze(0)\n",
    "\n",
    "    encoding = encode(hubert, wav, 7)\n",
    "    encoding = encoding.squeeze(0)\n",
    "    print(f\"encoding_shape: {encoding.shape}\")\n",
    "    print(f\"flags_shape: {len(flags)}\")\n",
    "    for w in range(max(wav_df[\"word_id\"])):\n",
    "        word_df = wav_df[wav_df[\"word_id\"] == w]\n",
    "        print(f\"word: {word_df['text'].iloc[0]}\")\n",
    "        word_boundaries = [word_df[\"word_start\"].iloc[0], word_df[\"word_end\"].iloc[0]]\n",
    "\n",
    "        print(f\"word_boundaries: {word_boundaries}\")\n",
    "        start_frame = get_frame_num(word_boundaries[0], 16000, 20)\n",
    "        end_frame = get_frame_num(word_boundaries[1], 16000, 20)\n",
    "\n",
    "        print(f\"frames: {start_frame, end_frame}\")\n",
    "        cut_encoding = encoding[start_frame:end_frame]\n",
    "        cut_flags = flags[start_frame:end_frame]\n",
    "\n",
    "        print(f\"cut_encoding_shape: {cut_encoding.shape}\")\n",
    "        print(f\"cut_flags_shape: {len(cut_flags)}\")\n",
    "        print(f\"cut_flags: {cut_flags}\")\n",
    "        clean_encoding = []\n",
    "        for i in range(min(cut_encoding.shape[0], len(flags))):\n",
    "            if cut_flags[i]:\n",
    "                clean_encoding.append(cut_encoding[i, :].unsqueeze(0))\n",
    "\n",
    "        if clean_encoding != []:\n",
    "            clean_encoding = torch.cat(clean_encoding, dim=0)\n",
    "\n",
    "        print(f\"clean_encoding_shape: {len(clean_encoding)}\")\n",
    "        codes = []\n",
    "        if clean_encoding != []:\n",
    "            codes, _ = segment(clean_encoding.numpy(), kmeans.cluster_centers_, 0.2)\n",
    "        print(f\"codes: {codes}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_dist_per_pair(pair):\n",
    "    \"\"\"\n",
    "    Calculates the normalized edit distance for a given pair of feature sequences.\n",
    "\n",
    "    Args:\n",
    "        chunk_pair (dict): Dictionary with a single key-value pair where:\n",
    "            - Key: Tuple (i, j) representing the indices of the feature pair.\n",
    "            - Value: Tuple (feature_i, feature_j) containing the feature sequences.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (index_i, index_j, normalized edit distance).\n",
    "    \"\"\"\n",
    "\n",
    "    id_1, id_2 = tuple(pair.keys())[0]\n",
    "    feature_1, feature_2 = tuple(pair.values())[0]\n",
    "\n",
    "    max_length = np.max([len(feature_1), len(feature_2)])\n",
    "    min_length = np.min([len(feature_1), len(feature_2)])\n",
    "    print(f\"max len {max_length}\")\n",
    "\n",
    "    dist = 0\n",
    "    if min_length == 0:\n",
    "        print(f\"{id_1, id_2}\\n{feature_1}\\n{feature_2}\\nDistance: {1.0}\")\n",
    "        return (id_1, id_2, 1.0)\n",
    "\n",
    "    if max_length > 0:\n",
    "        dist = editdistance.eval(feature_1, feature_2) / max_length\n",
    "\n",
    "    print(f\"{id_1, id_2}\\n{feature_1}\\n{feature_2}\\nDistance: {dist}\")\n",
    "    return (id_1, id_2, dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [40 13 28 62]\n",
      "1 [13 84 77 41]\n",
      "2 [16]\n",
      "3 [ 1 79 43 70 27 82]\n",
      "4 [16 90 25 43 70 51]\n",
      "5 [31 66 18 24]\n",
      "6 [13 43 51 20 68]\n",
      "7 [16]\n",
      "8 [32 74 45 74]\n",
      "9 [16 17 95 48 20 82 40 57 91  5 16]\n",
      "10 [65 28 86 10]\n",
      "11 [94 85 68 13]\n",
      "12 [17 95 48 20 82 40 57 91 16]\n",
      "13 [17 95 48 20 82 40 91  5 32]\n",
      "14 [13  8 43 71 52 26 55 77 41]\n",
      "15 [20 13  8 85 75 73 92 30 59]\n",
      "16 [17 95 48 20 82 40 91  5 32]\n",
      "17 []\n"
     ]
    }
   ],
   "source": [
    "# Process chunks\n",
    "filenames = []\n",
    "features = []\n",
    "# for feature in tqdm(paths, desc=\"Process Filenames\"):\n",
    "for i, path in enumerate(paths):\n",
    "    filenames.append(path.stem)\n",
    "    feature = np.load(path)\n",
    "    features.append(feature)\n",
    "    print(i, feature)\n",
    "\n",
    "\n",
    "# chunk_limit = 10\n",
    "# num_pairs = sample_size * (sample_size - 1) // 2\n",
    "# num_chunks = (num_pairs + chunk_limit - 1) // chunk_limit\n",
    "\n",
    "# row_indices = []\n",
    "# col_indices = []\n",
    "# values = []\n",
    "\n",
    "# for chunk in tqdm(\n",
    "#     get_batch_of_paths(sample_size, chunk_limit=chunk_limit),\n",
    "#     total=num_chunks,\n",
    "#     desc=\"Processing Chunks\",\n",
    "#     unit=\"chunk\",\n",
    "# ):\n",
    "#     chunk_units = [{(i, j): (features[i], features[j])} for i, j in chunk]\n",
    "#     chunk_results = []\n",
    "#     for pair in chunk_units:\n",
    "#         # print(pair)\n",
    "#         chunk_results.append(cal_dist_per_pair(pair))\n",
    "\n",
    "#     for i, j, dist in chunk_results:\n",
    "#         row_indices.append(i)\n",
    "#         col_indices.append(j)\n",
    "#         values.append(dist)\n",
    "\n",
    "#     dist_sparse = sp.coo_matrix(\n",
    "#         (values, (row_indices, col_indices)), shape=(sample_size, sample_size)\n",
    "#     )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
