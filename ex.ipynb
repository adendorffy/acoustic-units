{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from encode import sample_files\n",
    "from distance import get_batch_of_paths\n",
    "from tqdm import tqdm\n",
    "import scipy.sparse as sp\n",
    "import editdistance\n",
    "import numpy as np\n",
    "import igraph as ig\n",
    "import leidenalg as la\n",
    "import math\n",
    "from multiprocessing import Pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_dir = Path(\"data/dev-clean\")\n",
    "audio_ext = \".flac\"\n",
    "align_path = Path(\"data/alignments/dev-clean/alignments.csv\")\n",
    "save_dir = Path(\"features/\")\n",
    "wav_dir = Path(\"data/dev-clean\")\n",
    "feat_dir = Path(\"features/0.1\")\n",
    "\n",
    "align_df = pd.read_csv(align_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2703\n"
     ]
    }
   ],
   "source": [
    "paths, sample_size = sample_files(\n",
    "    audio_dir=audio_dir, audio_ext=audio_ext, sample_size=-1\n",
    ")\n",
    "# paths = [Path(\"data/dev-clean/174/50561/174-50561-0005.flac\")]\n",
    "sample_size = len(paths)\n",
    "print(sample_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63221\n"
     ]
    }
   ],
   "source": [
    "gamma = 0.1\n",
    "paths = (p for p in Path(f\"features/{gamma}\").rglob(\"*.npy\"))\n",
    "\n",
    "sorted_paths = sorted(paths, key=lambda x: int(x.stem.split(\"_\")[-1]))\n",
    "sample_size = len(sorted_paths)\n",
    "print(sample_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/danel/.cache/torch/hub/bshall_dusted_main\n",
      "Using cache found in /home/danel/.cache/torch/hub/bshall_dusted_main\n",
      "Using cache found in /home/danel/.cache/torch/hub/bshall_hubert_main\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "from webrtcvad import Vad\n",
    "from encode import mark_sil\n",
    "\n",
    "kmeans, segment = torch.hub.load(\n",
    "    \"bshall/dusted:main\", \"kmeans\", language=\"english\", trust_repo=True\n",
    ")\n",
    "hubert, encode = torch.hub.load(\n",
    "    \"bshall/dusted:main\", \"hubert\", language=\"english\", trust_repo=True\n",
    ")\n",
    "vad = Vad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Getting Units:   0%|          | 6/2703 [00:04<32:28,  1.38it/s]  \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 20\u001b[39m\n\u001b[32m     17\u001b[39m gamma = \u001b[32m0.1\u001b[39m\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mmax\u001b[39m(wav_df[\u001b[33m\"\u001b[39m\u001b[33mword_id\u001b[39m\u001b[33m\"\u001b[39m])):\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m     word_df = \u001b[43mwav_df\u001b[49m\u001b[43m[\u001b[49m\u001b[43mwav_df\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mword_id\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[43mw\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m     21\u001b[39m     word_boundaries = [word_df[\u001b[33m\"\u001b[39m\u001b[33mword_start\u001b[39m\u001b[33m\"\u001b[39m].iloc[\u001b[32m0\u001b[39m], word_df[\u001b[33m\"\u001b[39m\u001b[33mword_end\u001b[39m\u001b[33m\"\u001b[39m].iloc[\u001b[32m0\u001b[39m]]\n\u001b[32m     23\u001b[39m     start_frame = get_frame_num(word_boundaries[\u001b[32m0\u001b[39m], \u001b[32m16000\u001b[39m, \u001b[32m20\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/acoustic-units/.env/lib/python3.12/site-packages/pandas/core/frame.py:4093\u001b[39m, in \u001b[36mDataFrame.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   4091\u001b[39m \u001b[38;5;66;03m# Do we have a (boolean) 1d indexer?\u001b[39;00m\n\u001b[32m   4092\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m com.is_bool_indexer(key):\n\u001b[32m-> \u001b[39m\u001b[32m4093\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_getitem_bool_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4095\u001b[39m \u001b[38;5;66;03m# We are left with two options: a single key, and a collection of keys,\u001b[39;00m\n\u001b[32m   4096\u001b[39m \u001b[38;5;66;03m# We interpret tuples as collections only for non-MultiIndex\u001b[39;00m\n\u001b[32m   4097\u001b[39m is_single_key = \u001b[38;5;28misinstance\u001b[39m(key, \u001b[38;5;28mtuple\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_list_like(key)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/acoustic-units/.env/lib/python3.12/site-packages/pandas/core/frame.py:4155\u001b[39m, in \u001b[36mDataFrame._getitem_bool_array\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   4152\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.copy(deep=\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m   4154\u001b[39m indexer = key.nonzero()[\u001b[32m0\u001b[39m]\n\u001b[32m-> \u001b[39m\u001b[32m4155\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_take_with_is_copy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/acoustic-units/.env/lib/python3.12/site-packages/pandas/core/generic.py:4153\u001b[39m, in \u001b[36mNDFrame._take_with_is_copy\u001b[39m\u001b[34m(self, indices, axis)\u001b[39m\n\u001b[32m   4142\u001b[39m \u001b[38;5;129m@final\u001b[39m\n\u001b[32m   4143\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_take_with_is_copy\u001b[39m(\u001b[38;5;28mself\u001b[39m, indices, axis: Axis = \u001b[32m0\u001b[39m) -> Self:\n\u001b[32m   4144\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   4145\u001b[39m \u001b[33;03m    Internal version of the `take` method that sets the `_is_copy`\u001b[39;00m\n\u001b[32m   4146\u001b[39m \u001b[33;03m    attribute to keep track of the parent dataframe (using in indexing\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   4151\u001b[39m \u001b[33;03m    See the docstring of `take` for full explanation of the parameters.\u001b[39;00m\n\u001b[32m   4152\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m4153\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtake\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindices\u001b[49m\u001b[43m=\u001b[49m\u001b[43mindices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4154\u001b[39m     \u001b[38;5;66;03m# Maybe set copy if we didn't actually change the index.\u001b[39;00m\n\u001b[32m   4155\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.ndim == \u001b[32m2\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m result._get_axis(axis).equals(\u001b[38;5;28mself\u001b[39m._get_axis(axis)):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/acoustic-units/.env/lib/python3.12/site-packages/pandas/core/generic.py:4133\u001b[39m, in \u001b[36mNDFrame.take\u001b[39m\u001b[34m(self, indices, axis, **kwargs)\u001b[39m\n\u001b[32m   4128\u001b[39m     \u001b[38;5;66;03m# We can get here with a slice via DataFrame.__getitem__\u001b[39;00m\n\u001b[32m   4129\u001b[39m     indices = np.arange(\n\u001b[32m   4130\u001b[39m         indices.start, indices.stop, indices.step, dtype=np.intp\n\u001b[32m   4131\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m4133\u001b[39m new_data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_mgr\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtake\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4134\u001b[39m \u001b[43m    \u001b[49m\u001b[43mindices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4135\u001b[39m \u001b[43m    \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_block_manager_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4136\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverify\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   4137\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4138\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._constructor_from_mgr(new_data, axes=new_data.axes).__finalize__(\n\u001b[32m   4139\u001b[39m     \u001b[38;5;28mself\u001b[39m, method=\u001b[33m\"\u001b[39m\u001b[33mtake\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   4140\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/acoustic-units/.env/lib/python3.12/site-packages/pandas/core/internals/managers.py:894\u001b[39m, in \u001b[36mBaseBlockManager.take\u001b[39m\u001b[34m(self, indexer, axis, verify)\u001b[39m\n\u001b[32m    891\u001b[39m indexer = maybe_convert_indices(indexer, n, verify=verify)\n\u001b[32m    893\u001b[39m new_labels = \u001b[38;5;28mself\u001b[39m.axes[axis].take(indexer)\n\u001b[32m--> \u001b[39m\u001b[32m894\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mreindex_indexer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    895\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnew_axis\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnew_labels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    896\u001b[39m \u001b[43m    \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    897\u001b[39m \u001b[43m    \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    898\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_dups\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    899\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    900\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/acoustic-units/.env/lib/python3.12/site-packages/pandas/core/internals/managers.py:688\u001b[39m, in \u001b[36mBaseBlockManager.reindex_indexer\u001b[39m\u001b[34m(self, new_axis, indexer, axis, fill_value, allow_dups, copy, only_slice, use_na_proxy)\u001b[39m\n\u001b[32m    680\u001b[39m     new_blocks = \u001b[38;5;28mself\u001b[39m._slice_take_blocks_ax0(\n\u001b[32m    681\u001b[39m         indexer,\n\u001b[32m    682\u001b[39m         fill_value=fill_value,\n\u001b[32m    683\u001b[39m         only_slice=only_slice,\n\u001b[32m    684\u001b[39m         use_na_proxy=use_na_proxy,\n\u001b[32m    685\u001b[39m     )\n\u001b[32m    686\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    687\u001b[39m     new_blocks = [\n\u001b[32m--> \u001b[39m\u001b[32m688\u001b[39m         \u001b[43mblk\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtake_nd\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    689\u001b[39m \u001b[43m            \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    690\u001b[39m \u001b[43m            \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    691\u001b[39m \u001b[43m            \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    692\u001b[39m \u001b[43m                \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mblk\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfill_value\u001b[49m\n\u001b[32m    693\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    694\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    695\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m blk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.blocks\n\u001b[32m    696\u001b[39m     ]\n\u001b[32m    698\u001b[39m new_axes = \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m.axes)\n\u001b[32m    699\u001b[39m new_axes[axis] = new_axis\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/acoustic-units/.env/lib/python3.12/site-packages/pandas/core/internals/blocks.py:1307\u001b[39m, in \u001b[36mBlock.take_nd\u001b[39m\u001b[34m(self, indexer, axis, new_mgr_locs, fill_value)\u001b[39m\n\u001b[32m   1304\u001b[39m     allow_fill = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m   1306\u001b[39m \u001b[38;5;66;03m# Note: algos.take_nd has upcast logic similar to coerce_to_target_dtype\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1307\u001b[39m new_values = \u001b[43malgos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtake_nd\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1308\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_fill\u001b[49m\u001b[43m=\u001b[49m\u001b[43mallow_fill\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfill_value\u001b[49m\n\u001b[32m   1309\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1311\u001b[39m \u001b[38;5;66;03m# Called from three places in managers, all of which satisfy\u001b[39;00m\n\u001b[32m   1312\u001b[39m \u001b[38;5;66;03m#  these assertions\u001b[39;00m\n\u001b[32m   1313\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m, ExtensionBlock):\n\u001b[32m   1314\u001b[39m     \u001b[38;5;66;03m# NB: in this case, the 'axis' kwarg will be ignored in the\u001b[39;00m\n\u001b[32m   1315\u001b[39m     \u001b[38;5;66;03m#  algos.take_nd call above.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/acoustic-units/.env/lib/python3.12/site-packages/pandas/core/array_algos/take.py:117\u001b[39m, in \u001b[36mtake_nd\u001b[39m\u001b[34m(arr, indexer, axis, fill_value, allow_fill)\u001b[39m\n\u001b[32m    114\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m arr.take(indexer, fill_value=fill_value, allow_fill=allow_fill)\n\u001b[32m    116\u001b[39m arr = np.asarray(arr)\n\u001b[32m--> \u001b[39m\u001b[32m117\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_take_nd_ndarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_fill\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/acoustic-units/.env/lib/python3.12/site-packages/pandas/core/array_algos/take.py:133\u001b[39m, in \u001b[36m_take_nd_ndarray\u001b[39m\u001b[34m(arr, indexer, axis, fill_value, allow_fill)\u001b[39m\n\u001b[32m    130\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    131\u001b[39m     indexer = ensure_platform_int(indexer)\n\u001b[32m--> \u001b[39m\u001b[32m133\u001b[39m dtype, fill_value, mask_info = \u001b[43m_take_preprocess_indexer_and_fill_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    134\u001b[39m \u001b[43m    \u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_fill\u001b[49m\n\u001b[32m    135\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    137\u001b[39m flip_order = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    138\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m arr.ndim == \u001b[32m2\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m arr.flags.f_contiguous:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/acoustic-units/.env/lib/python3.12/site-packages/pandas/core/array_algos/take.py:586\u001b[39m, in \u001b[36m_take_preprocess_indexer_and_fill_value\u001b[39m\u001b[34m(arr, indexer, fill_value, allow_fill, mask)\u001b[39m\n\u001b[32m    584\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    585\u001b[39m     mask = indexer == -\u001b[32m1\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m586\u001b[39m     needs_masking = \u001b[38;5;28mbool\u001b[39m(\u001b[43mmask\u001b[49m\u001b[43m.\u001b[49m\u001b[43many\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    587\u001b[39m mask_info = mask, needs_masking\n\u001b[32m    588\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m needs_masking:\n\u001b[32m    589\u001b[39m     \u001b[38;5;66;03m# if not, then depromote, set fill_value to dummy\u001b[39;00m\n\u001b[32m    590\u001b[39m     \u001b[38;5;66;03m# (it won't be used but we don't want the cython code\u001b[39;00m\n\u001b[32m    591\u001b[39m     \u001b[38;5;66;03m# to crash when trying to cast it to dtype)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/acoustic-units/.env/lib/python3.12/site-packages/numpy/_core/_methods.py:59\u001b[39m, in \u001b[36m_any\u001b[39m\u001b[34m(a, axis, dtype, out, keepdims, where)\u001b[39m\n\u001b[32m     55\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_prod\u001b[39m(a, axis=\u001b[38;5;28;01mNone\u001b[39;00m, dtype=\u001b[38;5;28;01mNone\u001b[39;00m, out=\u001b[38;5;28;01mNone\u001b[39;00m, keepdims=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m     56\u001b[39m           initial=_NoValue, where=\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[32m     57\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m umr_prod(a, axis, dtype, out, keepdims, initial, where)\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_any\u001b[39m(a, axis=\u001b[38;5;28;01mNone\u001b[39;00m, dtype=\u001b[38;5;28;01mNone\u001b[39;00m, out=\u001b[38;5;28;01mNone\u001b[39;00m, keepdims=\u001b[38;5;28;01mFalse\u001b[39;00m, *, where=\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[32m     60\u001b[39m     \u001b[38;5;66;03m# By default, return a boolean for any and all\u001b[39;00m\n\u001b[32m     61\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     62\u001b[39m         dtype = bool_dt\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "def get_frame_num(timestamp: float, sample_rate: int, frame_size_ms: int) -> int:\n",
    "    hop = frame_size_ms / 1000 * sample_rate\n",
    "    hop_size = np.max([hop, 1])\n",
    "    return int((timestamp * sample_rate) / hop_size)\n",
    "\n",
    "\n",
    "for path in tqdm(paths, desc=\"Getting Units\"):\n",
    "    wav_df = align_df[align_df[\"filename\"] == path.stem]\n",
    "\n",
    "    wav, sr = torchaudio.load(str(path))\n",
    "    wav = torchaudio.functional.resample(wav, sr, 16000)\n",
    "    flags = mark_sil(vad, wav)\n",
    "    wav = wav.unsqueeze(0)\n",
    "\n",
    "    encoding = encode(hubert, wav, 7)\n",
    "    encoding = encoding.squeeze(0)\n",
    "    gamma = 0.1\n",
    "\n",
    "    for w in range(max(wav_df[\"word_id\"])):\n",
    "        word_df = wav_df[wav_df[\"word_id\"] == w]\n",
    "        word_boundaries = [word_df[\"word_start\"].iloc[0], word_df[\"word_end\"].iloc[0]]\n",
    "\n",
    "        start_frame = get_frame_num(word_boundaries[0], 16000, 20)\n",
    "        end_frame = get_frame_num(word_boundaries[1], 16000, 20)\n",
    "\n",
    "        cut_encoding = encoding[start_frame:end_frame]\n",
    "        cut_flags = flags[start_frame:end_frame]\n",
    "        clean_encoding = []\n",
    "        for i in range(min(cut_encoding.shape[0], len(flags))):\n",
    "            if cut_flags[i]:\n",
    "                clean_encoding.append(cut_encoding[i, :].unsqueeze(0))\n",
    "\n",
    "        if clean_encoding != []:\n",
    "            clean_encoding = torch.cat(clean_encoding, dim=0)\n",
    "\n",
    "        codes = []\n",
    "        if clean_encoding != []:\n",
    "            codes, _ = segment(\n",
    "                clean_encoding.numpy(), kmeans.cluster_centers_, gamma=gamma\n",
    "            )\n",
    "\n",
    "            save_path = (\n",
    "                Path(\"features\")\n",
    "                / str(gamma)\n",
    "                / path.relative_to(wav_dir)\n",
    "                / f\"{path.stem}_{w}.npy\"\n",
    "            )\n",
    "            save_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "            np.save(save_path, codes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_dist_per_pair(pair):\n",
    "    \"\"\"\n",
    "    Calculates the normalized edit distance for a given pair of feature sequences.\n",
    "\n",
    "    Args:\n",
    "        chunk_pair (dict): Dictionary with a single key-value pair where:\n",
    "            - Key: Tuple (i, j) representing the indices of the feature pair.\n",
    "            - Value: Tuple (feature_i, feature_j) containing the feature sequences.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (index_i, index_j, normalized edit distance).\n",
    "    \"\"\"\n",
    "\n",
    "    id_1, id_2 = tuple(pair.keys())[0]\n",
    "    feature_1, feature_2 = tuple(pair.values())[0]\n",
    "\n",
    "    max_length = np.max([len(feature_1), len(feature_2)])\n",
    "    min_length = np.min([len(feature_1), len(feature_2)])\n",
    "    # print(f\"max len {max_length}\")\n",
    "\n",
    "    dist = 0\n",
    "    if min_length == 0:\n",
    "        # print(f\"{id_1, id_2}\\n{feature_1}\\n{feature_2}\\nDistance: {1.0}\\n\")\n",
    "        return (id_1, id_2, 1.0)\n",
    "\n",
    "    if max_length > 0:\n",
    "        dist = editdistance.eval(feature_1, feature_2) / max_length\n",
    "\n",
    "    # if dist < 0.5:\n",
    "    # print(f\"{id_1, id_2}\\n{feature_1}\\n{feature_2}\\nDistance: {dist}\\n\")\n",
    "    return (id_1, id_2, dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Appending Features: 100%|██████████| 63137/63137 [00:16<00:00, 3887.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_pairs: 1993108816\n",
      "num_chunks: 399\n",
      "num_samples: 63137\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Chunks:   0%|          | 0/399 [00:00<?, ?chunk/s]Process SpawnPoolWorker-7:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/daneladendorff/.pyenv/versions/3.12.8/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/daneladendorff/.pyenv/versions/3.12.8/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/daneladendorff/.pyenv/versions/3.12.8/lib/python3.12/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "           ^^^^^\n",
      "  File \"/Users/daneladendorff/.pyenv/versions/3.12.8/lib/python3.12/multiprocessing/queues.py\", line 389, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'cal_dist_per_pair' on <module '__main__' (<class '_frozen_importlib.BuiltinImporter'>)>\n",
      "Process SpawnPoolWorker-12:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/daneladendorff/.pyenv/versions/3.12.8/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/daneladendorff/.pyenv/versions/3.12.8/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/daneladendorff/.pyenv/versions/3.12.8/lib/python3.12/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "           ^^^^^\n",
      "  File \"/Users/daneladendorff/.pyenv/versions/3.12.8/lib/python3.12/multiprocessing/queues.py\", line 389, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'cal_dist_per_pair' on <module '__main__' (<class '_frozen_importlib.BuiltinImporter'>)>\n",
      "Process SpawnPoolWorker-8:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/daneladendorff/.pyenv/versions/3.12.8/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/daneladendorff/.pyenv/versions/3.12.8/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/daneladendorff/.pyenv/versions/3.12.8/lib/python3.12/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "           ^^^^^\n",
      "  File \"/Users/daneladendorff/.pyenv/versions/3.12.8/lib/python3.12/multiprocessing/queues.py\", line 389, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'cal_dist_per_pair' on <module '__main__' (<class '_frozen_importlib.BuiltinImporter'>)>\n",
      "Process SpawnPoolWorker-11:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/daneladendorff/.pyenv/versions/3.12.8/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/daneladendorff/.pyenv/versions/3.12.8/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/daneladendorff/.pyenv/versions/3.12.8/lib/python3.12/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "           ^^^^^\n",
      "  File \"/Users/daneladendorff/.pyenv/versions/3.12.8/lib/python3.12/multiprocessing/queues.py\", line 389, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'cal_dist_per_pair' on <module '__main__' (<class '_frozen_importlib.BuiltinImporter'>)>\n",
      "Process SpawnPoolWorker-9:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/daneladendorff/.pyenv/versions/3.12.8/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/daneladendorff/.pyenv/versions/3.12.8/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/daneladendorff/.pyenv/versions/3.12.8/lib/python3.12/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "           ^^^^^\n",
      "  File \"/Users/daneladendorff/.pyenv/versions/3.12.8/lib/python3.12/multiprocessing/queues.py\", line 389, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'cal_dist_per_pair' on <module '__main__' (<class '_frozen_importlib.BuiltinImporter'>)>\n",
      "Process SpawnPoolWorker-10:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/daneladendorff/.pyenv/versions/3.12.8/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/daneladendorff/.pyenv/versions/3.12.8/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/daneladendorff/.pyenv/versions/3.12.8/lib/python3.12/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "           ^^^^^\n",
      "  File \"/Users/daneladendorff/.pyenv/versions/3.12.8/lib/python3.12/multiprocessing/queues.py\", line 389, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'cal_dist_per_pair' on <module '__main__' (<class '_frozen_importlib.BuiltinImporter'>)>\n",
      "Process SpawnPoolWorker-13:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/daneladendorff/.pyenv/versions/3.12.8/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/daneladendorff/.pyenv/versions/3.12.8/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/daneladendorff/.pyenv/versions/3.12.8/lib/python3.12/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "           ^^^^^\n",
      "  File \"/Users/daneladendorff/.pyenv/versions/3.12.8/lib/python3.12/multiprocessing/queues.py\", line 389, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'cal_dist_per_pair' on <module '__main__' (<class '_frozen_importlib.BuiltinImporter'>)>\n",
      "Process SpawnPoolWorker-14:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/daneladendorff/.pyenv/versions/3.12.8/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/daneladendorff/.pyenv/versions/3.12.8/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/daneladendorff/.pyenv/versions/3.12.8/lib/python3.12/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "           ^^^^^\n",
      "  File \"/Users/daneladendorff/.pyenv/versions/3.12.8/lib/python3.12/multiprocessing/queues.py\", line 389, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'cal_dist_per_pair' on <module '__main__' (<class '_frozen_importlib.BuiltinImporter'>)>\n",
      "Process SpawnPoolWorker-15:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/daneladendorff/.pyenv/versions/3.12.8/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/daneladendorff/.pyenv/versions/3.12.8/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/daneladendorff/.pyenv/versions/3.12.8/lib/python3.12/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "           ^^^^^\n",
      "  File \"/Users/daneladendorff/.pyenv/versions/3.12.8/lib/python3.12/multiprocessing/queues.py\", line 389, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'cal_dist_per_pair' on <module '__main__' (<class '_frozen_importlib.BuiltinImporter'>)>\n",
      "Process SpawnPoolWorker-16:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/daneladendorff/.pyenv/versions/3.12.8/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/daneladendorff/.pyenv/versions/3.12.8/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/daneladendorff/.pyenv/versions/3.12.8/lib/python3.12/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "           ^^^^^\n",
      "  File \"/Users/daneladendorff/.pyenv/versions/3.12.8/lib/python3.12/multiprocessing/queues.py\", line 389, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'cal_dist_per_pair' on <module '__main__' (<class '_frozen_importlib.BuiltinImporter'>)>\n",
      "Process SpawnPoolWorker-17:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/daneladendorff/.pyenv/versions/3.12.8/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/daneladendorff/.pyenv/versions/3.12.8/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/daneladendorff/.pyenv/versions/3.12.8/lib/python3.12/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "           ^^^^^\n",
      "  File \"/Users/daneladendorff/.pyenv/versions/3.12.8/lib/python3.12/multiprocessing/queues.py\", line 389, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'cal_dist_per_pair' on <module '__main__' (<class '_frozen_importlib.BuiltinImporter'>)>\n",
      "Process SpawnPoolWorker-18:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/daneladendorff/.pyenv/versions/3.12.8/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/daneladendorff/.pyenv/versions/3.12.8/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/daneladendorff/.pyenv/versions/3.12.8/lib/python3.12/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "           ^^^^^\n",
      "  File \"/Users/daneladendorff/.pyenv/versions/3.12.8/lib/python3.12/multiprocessing/queues.py\", line 389, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'cal_dist_per_pair' on <module '__main__' (<class '_frozen_importlib.BuiltinImporter'>)>\n",
      "Process SpawnPoolWorker-19:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/daneladendorff/.pyenv/versions/3.12.8/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/daneladendorff/.pyenv/versions/3.12.8/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/daneladendorff/.pyenv/versions/3.12.8/lib/python3.12/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "           ^^^^^\n",
      "  File \"/Users/daneladendorff/.pyenv/versions/3.12.8/lib/python3.12/multiprocessing/queues.py\", line 389, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'cal_dist_per_pair' on <module '__main__' (<class '_frozen_importlib.BuiltinImporter'>)>\n",
      "Process SpawnPoolWorker-20:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/daneladendorff/.pyenv/versions/3.12.8/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/daneladendorff/.pyenv/versions/3.12.8/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/daneladendorff/.pyenv/versions/3.12.8/lib/python3.12/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "           ^^^^^\n",
      "  File \"/Users/daneladendorff/.pyenv/versions/3.12.8/lib/python3.12/multiprocessing/queues.py\", line 389, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'cal_dist_per_pair' on <module '__main__' (<class '_frozen_importlib.BuiltinImporter'>)>\n",
      "Process SpawnPoolWorker-21:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/daneladendorff/.pyenv/versions/3.12.8/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/daneladendorff/.pyenv/versions/3.12.8/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/daneladendorff/.pyenv/versions/3.12.8/lib/python3.12/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "           ^^^^^\n",
      "  File \"/Users/daneladendorff/.pyenv/versions/3.12.8/lib/python3.12/multiprocessing/queues.py\", line 389, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'cal_dist_per_pair' on <module '__main__' (<class '_frozen_importlib.BuiltinImporter'>)>\n",
      "Process SpawnPoolWorker-22:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/daneladendorff/.pyenv/versions/3.12.8/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/daneladendorff/.pyenv/versions/3.12.8/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/daneladendorff/.pyenv/versions/3.12.8/lib/python3.12/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "           ^^^^^\n",
      "  File \"/Users/daneladendorff/.pyenv/versions/3.12.8/lib/python3.12/multiprocessing/queues.py\", line 389, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'cal_dist_per_pair' on <module '__main__' (<class '_frozen_importlib.BuiltinImporter'>)>\n",
      "Process SpawnPoolWorker-23:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/daneladendorff/.pyenv/versions/3.12.8/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/daneladendorff/.pyenv/versions/3.12.8/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/daneladendorff/.pyenv/versions/3.12.8/lib/python3.12/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "           ^^^^^\n",
      "  File \"/Users/daneladendorff/.pyenv/versions/3.12.8/lib/python3.12/multiprocessing/queues.py\", line 389, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'cal_dist_per_pair' on <module '__main__' (<class '_frozen_importlib.BuiltinImporter'>)>\n",
      "Process SpawnPoolWorker-24:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/daneladendorff/.pyenv/versions/3.12.8/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/daneladendorff/.pyenv/versions/3.12.8/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/daneladendorff/.pyenv/versions/3.12.8/lib/python3.12/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "           ^^^^^\n",
      "  File \"/Users/daneladendorff/.pyenv/versions/3.12.8/lib/python3.12/multiprocessing/queues.py\", line 389, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'cal_dist_per_pair' on <module '__main__' (<class '_frozen_importlib.BuiltinImporter'>)>\n",
      "Process SpawnPoolWorker-25:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/daneladendorff/.pyenv/versions/3.12.8/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/daneladendorff/.pyenv/versions/3.12.8/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/daneladendorff/.pyenv/versions/3.12.8/lib/python3.12/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "           ^^^^^\n",
      "  File \"/Users/daneladendorff/.pyenv/versions/3.12.8/lib/python3.12/multiprocessing/queues.py\", line 389, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'cal_dist_per_pair' on <module '__main__' (<class '_frozen_importlib.BuiltinImporter'>)>\n",
      "Process SpawnPoolWorker-26:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/daneladendorff/.pyenv/versions/3.12.8/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/daneladendorff/.pyenv/versions/3.12.8/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/daneladendorff/.pyenv/versions/3.12.8/lib/python3.12/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "           ^^^^^\n",
      "  File \"/Users/daneladendorff/.pyenv/versions/3.12.8/lib/python3.12/multiprocessing/queues.py\", line 389, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'cal_dist_per_pair' on <module '__main__' (<class '_frozen_importlib.BuiltinImporter'>)>\n",
      "Process SpawnPoolWorker-27:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/daneladendorff/.pyenv/versions/3.12.8/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/daneladendorff/.pyenv/versions/3.12.8/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/daneladendorff/.pyenv/versions/3.12.8/lib/python3.12/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "           ^^^^^\n",
      "  File \"/Users/daneladendorff/.pyenv/versions/3.12.8/lib/python3.12/multiprocessing/queues.py\", line 389, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'cal_dist_per_pair' on <module '__main__' (<class '_frozen_importlib.BuiltinImporter'>)>\n",
      "Process SpawnPoolWorker-28:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/daneladendorff/.pyenv/versions/3.12.8/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/daneladendorff/.pyenv/versions/3.12.8/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/daneladendorff/.pyenv/versions/3.12.8/lib/python3.12/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "           ^^^^^\n",
      "  File \"/Users/daneladendorff/.pyenv/versions/3.12.8/lib/python3.12/multiprocessing/queues.py\", line 389, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'cal_dist_per_pair' on <module '__main__' (<class '_frozen_importlib.BuiltinImporter'>)>\n",
      "Process SpawnPoolWorker-29:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/daneladendorff/.pyenv/versions/3.12.8/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/daneladendorff/.pyenv/versions/3.12.8/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/daneladendorff/.pyenv/versions/3.12.8/lib/python3.12/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "           ^^^^^\n",
      "  File \"/Users/daneladendorff/.pyenv/versions/3.12.8/lib/python3.12/multiprocessing/queues.py\", line 389, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'cal_dist_per_pair' on <module '__main__' (<class '_frozen_importlib.BuiltinImporter'>)>\n",
      "Process SpawnPoolWorker-30:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/daneladendorff/.pyenv/versions/3.12.8/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/daneladendorff/.pyenv/versions/3.12.8/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/daneladendorff/.pyenv/versions/3.12.8/lib/python3.12/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "           ^^^^^\n",
      "  File \"/Users/daneladendorff/.pyenv/versions/3.12.8/lib/python3.12/multiprocessing/queues.py\", line 389, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'cal_dist_per_pair' on <module '__main__' (<class '_frozen_importlib.BuiltinImporter'>)>\n",
      "Process SpawnPoolWorker-34:\n",
      "Process SpawnPoolWorker-36:\n",
      "Process SpawnPoolWorker-35:\n",
      "Process SpawnPoolWorker-32:\n",
      "Process SpawnPoolWorker-31:\n",
      "Process SpawnPoolWorker-33:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Processing Chunks:   0%|          | 0/399 [01:05<?, ?chunk/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 38\u001b[0m\n\u001b[1;32m     36\u001b[0m chunk_results \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Pool(\u001b[38;5;241m6\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m pool:\n\u001b[0;32m---> 38\u001b[0m     chunk_results \u001b[38;5;241m=\u001b[39m \u001b[43mpool\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcal_dist_per_pair\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk_units\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, j, dist \u001b[38;5;129;01min\u001b[39;00m chunk_results:\n\u001b[1;32m     41\u001b[0m     row_indices\u001b[38;5;241m.\u001b[39mappend(i)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.8/lib/python3.12/multiprocessing/pool.py:367\u001b[0m, in \u001b[0;36mPool.map\u001b[0;34m(self, func, iterable, chunksize)\u001b[0m\n\u001b[1;32m    362\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mmap\u001b[39m(\u001b[38;5;28mself\u001b[39m, func, iterable, chunksize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    363\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m    364\u001b[0m \u001b[38;5;124;03m    Apply `func` to each element in `iterable`, collecting the results\u001b[39;00m\n\u001b[1;32m    365\u001b[0m \u001b[38;5;124;03m    in a list that is returned.\u001b[39;00m\n\u001b[1;32m    366\u001b[0m \u001b[38;5;124;03m    '''\u001b[39;00m\n\u001b[0;32m--> 367\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_map_async\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapstar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.8/lib/python3.12/multiprocessing/pool.py:768\u001b[0m, in \u001b[0;36mApplyResult.get\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    767\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mget\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 768\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    769\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mready():\n\u001b[1;32m    770\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.8/lib/python3.12/multiprocessing/pool.py:765\u001b[0m, in \u001b[0;36mApplyResult.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    764\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwait\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 765\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_event\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.8/lib/python3.12/threading.py:655\u001b[0m, in \u001b[0;36mEvent.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    653\u001b[0m signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flag\n\u001b[1;32m    654\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m signaled:\n\u001b[0;32m--> 655\u001b[0m     signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cond\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    656\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m signaled\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.8/lib/python3.12/threading.py:355\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    353\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    354\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 355\u001b[0m         \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    356\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    357\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Process chunks\n",
    "gamma = 0.1\n",
    "paths = list(Path(f\"features/{gamma}\").rglob(\"*.npy\"))\n",
    "sample_size = len(paths)\n",
    "sorted_paths = sorted(paths, key=lambda x: int(x.stem.split(\"_\")[-1]))\n",
    "\n",
    "filenames = []\n",
    "features = []\n",
    "# for feature in tqdm(paths, desc=\"Process Filenames\"):\n",
    "for path in tqdm(sorted_paths, desc=\"Appending Features\"):\n",
    "    word_id = path.stem.split(\"_\")[1]\n",
    "    filenames.append(path.stem)\n",
    "    feature = np.load(path)\n",
    "    features.append(feature)\n",
    "\n",
    "\n",
    "chunk_limit = 5000000\n",
    "num_pairs = sample_size * (sample_size - 1) // 2\n",
    "num_chunks = (num_pairs + chunk_limit - 1) // chunk_limit\n",
    "\n",
    "row_indices = []\n",
    "col_indices = []\n",
    "values = []\n",
    "\n",
    "print(f\"num_pairs: {num_pairs}\")\n",
    "print(f\"num_chunks: {num_chunks}\")\n",
    "print(f\"num_samples: {sample_size}\")\n",
    "\n",
    "for chunk in tqdm(\n",
    "    get_batch_of_paths(sample_size, chunk_limit=chunk_limit),\n",
    "    total=num_chunks,\n",
    "    desc=\"Processing Chunks\",\n",
    "    unit=\"chunk\",\n",
    "):\n",
    "    chunk_units = [{(i, j): (features[i], features[j])} for i, j in chunk]\n",
    "    chunk_results = []\n",
    "    with Pool(6) as pool:\n",
    "        chunk_results = pool.map(cal_dist_per_pair, chunk_units)\n",
    "\n",
    "    for i, j, dist in chunk_results:\n",
    "        row_indices.append(i)\n",
    "        col_indices.append(j)\n",
    "        values.append(dist)\n",
    "\n",
    "    dist_sparse = sp.coo_matrix(\n",
    "        (values, (row_indices, col_indices)), shape=(sample_size, sample_size)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating total:   0%|          | 0/400 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating total: 100%|██████████| 400/400 [00:27<00:00, 14.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1998415810\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "chunk_idx = 400\n",
    "gamma = 0.1\n",
    "elements = [\"rows\", \"cols\", \"vals\"]\n",
    "first_chunk = np.load(f\"output/{gamma}/temp/temp_rows_0.npy\")\n",
    "dtype = first_chunk.dtype\n",
    "total_size = sum(\n",
    "    np.load(f\"output/{gamma}/temp/temp_rows_{i}.npy\").shape[0]\n",
    "    for i in tqdm(range(chunk_idx), desc=\"Calculating total\")\n",
    ")\n",
    "print(total_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for element in [\"rows\"]:\n",
    "    file_path = f\"output/{gamma}/{element}.npy\"\n",
    "\n",
    "    # Preallocate memory-mapped file\n",
    "    merged_data = np.memmap(file_path, dtype=dtype, mode=\"w+\", shape=(total_size,))\n",
    "\n",
    "    index = 0\n",
    "    for i in tqdm(range(chunk_idx), desc=f\"Writing {element} to disk\"):\n",
    "        temp_data = np.load(f\"output/{gamma}/temp/temp_{element}_{i}.npy\")\n",
    "        merged_data[index : index + temp_data.shape[0]] = temp_data\n",
    "        if i >= 399:\n",
    "            print(f\"{index}:{index + temp_data.shape[0]}\")\n",
    "        index += temp_data.shape[0]\n",
    "\n",
    "    merged_data.flush()\n",
    "    del merged_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Appending Text: 100%|██████████| 63221/63221 [03:39<00:00, 288.59it/s]\n"
     ]
    }
   ],
   "source": [
    "gamma = 0.1\n",
    "paths = (p for p in Path(f\"features/{gamma}\").rglob(\"*.npy\"))\n",
    "align_df = pd.read_csv(\"data/alignments/dev-clean/alignments.csv\")\n",
    "sorted_paths = sorted(paths, key=lambda x: int(x.stem.split(\"_\")[-1]))\n",
    "\n",
    "text = []\n",
    "for path in tqdm(sorted_paths, desc=\"Appending Text\"):\n",
    "    filename_parts = path.stem.split(\"_\")\n",
    "    wav_df = align_df[align_df[\"filename\"] == filename_parts[0]]\n",
    "    word_df = wav_df[wav_df[\"word_id\"] == int(filename_parts[1])]\n",
    "    text.append(str(word_df[\"text\"].iloc[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating total: 100%|██████████| 400/400 [00:02<00:00, 145.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_size: 1998415810, sample_size: 63221\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "gamma = 0.1\n",
    "temp_dir = Path(f\"output/{gamma}/temp\")\n",
    "\n",
    "\n",
    "def get_total_size(temp_dir, total_chunks):\n",
    "    total_size = sum(\n",
    "        np.load(temp_dir / f\"temp_rows_{i}.npy\").shape[0]\n",
    "        for i in tqdm(range(total_chunks), desc=\"Calculating total\")\n",
    "    )\n",
    "    return total_size\n",
    "\n",
    "\n",
    "def get_n(length_list):\n",
    "    return int(1 + np.sqrt(1 + 8 * length_list)) // 2\n",
    "\n",
    "\n",
    "total_size = get_total_size(temp_dir, 400)\n",
    "sample_size = get_n(total_size)\n",
    "print(f\"total_size: {total_size}, sample_size: {sample_size}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Getting Temp Info: 100%|██████████| 400/400 [02:29<00:00,  2.67it/s]\n"
     ]
    }
   ],
   "source": [
    "g = ig.Graph()\n",
    "g.add_vertices(sample_size)\n",
    "\n",
    "chunks = 400\n",
    "for i in tqdm(range(chunks), desc=\"Getting Temp Info\"):\n",
    "    temp_rows = np.load(temp_dir / f\"temp_rows_{i}.npy\")\n",
    "    temp_cols = np.load(temp_dir / f\"temp_cols_{i}.npy\")\n",
    "    temp_vals = np.load(temp_dir / f\"temp_vals_{i}.npy\")\n",
    "\n",
    "    mask = temp_vals < 0.4\n",
    "    filtered_rows = temp_rows[mask]\n",
    "    filtered_cols = temp_cols[mask]\n",
    "    filtered_vals = temp_vals[mask]\n",
    "\n",
    "    # Convert edges and weights to lists\n",
    "    edges = list(zip(map(int, filtered_rows), map(int, filtered_cols)))\n",
    "    weights = list(map(float, filtered_vals))\n",
    "\n",
    "    # Add edges if they exist\n",
    "    if edges:\n",
    "        g.add_edges(edges)\n",
    "\n",
    "        # Assign weights only to newly added edges\n",
    "        if weights:\n",
    "            g.es[-len(weights) :].set_attribute_values(\"weight\", weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filtering Weights: 100%|██████████| 5752547/5752547 [00:02<00:00, 2052098.24it/s]\n"
     ]
    }
   ],
   "source": [
    "num_clusters = 13967\n",
    "for e in tqdm(g.es, desc=\"Filtering Weights\"):\n",
    "    if e[\"weight\"] <= 0:\n",
    "        e[\"weight\"] = 1e-10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Graph.summary of <igraph.Graph object at 0x7c2cbbb85050>>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g.summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "g.write_pickle(f\"output/{gamma}/graph.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimising Resolution:  20%|██        | 1/5 [00:17<01:11, 17.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resolution 0.025: 13800 clusters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimising Resolution:  40%|████      | 2/5 [00:35<00:52, 17.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resolution 0.026: 13908 clusters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimising Resolution:  60%|██████    | 3/5 [00:53<00:35, 17.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resolution 0.027: 13902 clusters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimising Resolution:  80%|████████  | 4/5 [01:10<00:17, 17.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resolution 0.028: 14075 clusters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimising Resolution: 100%|██████████| 5/5 [01:29<00:00, 17.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resolution 0.029: 14135 clusters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "num_clusters = 13967  # Desired number of clusters\n",
    "\n",
    "for res in tqdm([0.025, 0.026, 0.027, 0.028, 0.029], desc=\"Optimising Resolution\"):\n",
    "    partition = la.find_partition(\n",
    "        g, la.CPMVertexPartition, weights=\"weight\", resolution_parameter=res\n",
    "    )\n",
    "    actual_clusters = len(set(partition.membership))\n",
    "    print(f\"Resolution {res}: {actual_clusters} clusters\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster difference: -3\n"
     ]
    }
   ],
   "source": [
    "num_clusters = 13967\n",
    "partition = la.find_partition(\n",
    "    g, la.CPMVertexPartition, weights=\"weight\", resolution_parameter=0.0277\n",
    ")\n",
    "actual_clusters = len(set(partition.membership))\n",
    "print(f\"Cluster difference: {abs(actual_clusters - num_clusters)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary to store cluster assignments\n",
    "clusters = {i: [] for i in set(partition.membership)}\n",
    "\n",
    "# Assign nodes to clusters\n",
    "for node, cluster_id in enumerate(partition.membership):\n",
    "    clusters[cluster_id].append(node)  # Append node index to the respective cluster\n",
    "\n",
    "cluster_transcriptions = []\n",
    "for cluster_id, words in clusters.items():\n",
    "    for w in words:\n",
    "        cluster_transcriptions.append((cluster_id, text[w]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NED: 0.07928736711516866\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "import statistics\n",
    "\n",
    "\n",
    "def distance(p, q):\n",
    "    length = max(len(p), len(q))\n",
    "    return editdistance.eval(p, q) / length if length > 0 else 1\n",
    "\n",
    "\n",
    "def ned(discovered):\n",
    "    discovered = sorted(discovered, key=lambda x: x[0])\n",
    "    distances = [\n",
    "        distance(p[1], q[1])\n",
    "        for _, group in itertools.groupby(discovered, key=lambda x: x[0])\n",
    "        for p, q in itertools.combinations(group, 2)\n",
    "    ]\n",
    "    return statistics.mean(distances)\n",
    "\n",
    "\n",
    "print(f\"NED: {ned(cluster_transcriptions)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 0.1\n",
    "rows = np.memmap(f\"output/{gamma}/rows.npy\", mode=\"r\", shape=(total_size,))\n",
    "cols = np.memmap(f\"output/{gamma}/cols.npy\", mode=\"r\", shape=(total_size,))\n",
    "vals = np.memmap(f\"output/{gamma}/vals.npy\", mode=\"r\", shape=(total_size,))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample_size: 63221\n"
     ]
    }
   ],
   "source": [
    "def get_n(length_list):\n",
    "    return int(1 + np.sqrt(1 + 8 * length_list)) // 2\n",
    "\n",
    "\n",
    "print(f\"sample_size: {get_n(total_size)}\")\n",
    "sample_size = get_n(total_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = ig.Graph()\n",
    "g.add_vertices(sample_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a boolean mask for values < 0.4\n",
    "mask = vals < 0.4\n",
    "\n",
    "# Directly create edges as tuples (node1, node2, weight)\n",
    "edges_with_weights = np.column_stack((rows[mask], cols[mask], vals[mask]))\n",
    "\n",
    "# Convert to list of tuples for NetworkX\n",
    "edges_with_weights = [\n",
    "    tuple(edge) for edge in tqdm(edges_with_weights, desc=\"Creating Edges with Weights\")\n",
    "]\n",
    "\n",
    "# Add all edges to the graph in one batch operation\n",
    "g.add_edges_from(edges_with_weights, weight=\"weight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample_size: 5609\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filtering edges: 100%|██████████| 15727680/15727680 [00:03<00:00, 3987707.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "351\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding edges: 100%|██████████| 351/351 [00:00<00:00, 39821.50it/s]\n",
      "Assigning weights: 100%|██████████| 351/351 [00:00<00:00, 1111094.87it/s]\n"
     ]
    }
   ],
   "source": [
    "def num_vertices_from_upper_triangular(num_points):\n",
    "    return int((1 + math.sqrt(1 + 8 * num_points)) / 2)\n",
    "\n",
    "\n",
    "# Get the number of vertices\n",
    "num_data_points = len(dist_sparse.data)\n",
    "sample_size = num_vertices_from_upper_triangular(num_data_points)\n",
    "print(f\"sample_size: {sample_size}\")\n",
    "# Initialize graph\n",
    "g = ig.Graph()\n",
    "g.add_vertices(sample_size)\n",
    "\n",
    "# Extract rows, cols, and values from COO format (efficiently)\n",
    "rows, cols, vals = dist_sparse.row, dist_sparse.col, dist_sparse.data\n",
    "\n",
    "# Filter edges and weights efficiently\n",
    "filtered_edges = [\n",
    "    (r, c)\n",
    "    for r, c, v in tqdm(\n",
    "        zip(rows, cols, vals), total=len(dist_sparse.data), desc=\"Filtering edges\"\n",
    "    )\n",
    "    if v > 0 and v < 0.4\n",
    "]\n",
    "filtered_weights = [v for v in vals if v > 0 and v < 0.4]\n",
    "\n",
    "print(len(filtered_edges))\n",
    "\n",
    "# Add edges with progress tracking\n",
    "for idx, edge in tqdm(\n",
    "    enumerate(filtered_edges),\n",
    "    desc=\"Adding edges\",\n",
    "    total=len(filtered_edges),\n",
    "    mininterval=1.0,\n",
    "):\n",
    "    g.add_edge(*edge, weight=filtered_weights[idx])\n",
    "\n",
    "# Add weights with progress tracking\n",
    "for idx, weight in tqdm(\n",
    "    enumerate(filtered_weights), desc=\"Assigning weights\", total=len(filtered_weights)\n",
    "):\n",
    "    g.es[idx][\"weight\"] = weight\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18\n"
     ]
    }
   ],
   "source": [
    "print(len(g.vs))\n",
    "communities = g.community_edge_betweenness()\n",
    "communities = communities.as_clustering()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clustering with 18 elements and 14 clusters\n",
      "[ 0] 1, 3, 8, 17\n",
      "[ 1] 2, 4\n",
      "[ 2] 0\n",
      "[ 3] 5\n",
      "[ 4] 6\n",
      "[ 5] 7\n",
      "[ 6] 9\n",
      "[ 7] 10\n",
      "[ 8] 11\n",
      "[ 9] 12\n",
      "[10] 13\n",
      "[11] 14\n",
      "[12] 15\n",
      "[13] 16\n"
     ]
    }
   ],
   "source": [
    "partition = la.find_partition(g, la.ModularityVertexPartition)\n",
    "print(partition)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
