{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from utils.features import DataSet\n",
    "\n",
    "name = \"librispeech-dev-clean\"\n",
    "in_dir = Path(\"data/dev-clean\")\n",
    "align_dir = Path(\"data/alignments/dev-clean\")\n",
    "feat_dir = Path(\"features\")\n",
    "audio_ext = \".flac\" \n",
    "\n",
    "dataset = DataSet(\n",
    "    name, in_dir, align_dir, feat_dir, audio_ext \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63137\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "audio_paths = list(dataset.in_dir.rglob(f\"**/*{dataset.audio_ext}\"))\n",
    "sample_feature_paths = list(Path(dataset.feat_dir / \"dusted_units/0.2/\").rglob(\"**/*.npy\"))\n",
    "\n",
    "sample = False\n",
    "sample_size = None\n",
    "\n",
    "if sample: \n",
    "    # sample_audio_paths = random.sample(audio_paths, sample_size)\n",
    "    sample_feature_paths = random.sample(sample_feature_paths, sample_size)\n",
    "    # sample_feature_paths = feature_paths[0:sample_size]\n",
    "\n",
    "sample_size = len(sample_feature_paths)\n",
    "file_map = {}\n",
    "for i, feature in enumerate(sample_feature_paths):\n",
    "    file_map[i] = feature\n",
    "\n",
    "print(len(sample_feature_paths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pair_generator(num_paths):\n",
    "    for i in range(num_paths):\n",
    "        for j in range(i + 1, num_paths):\n",
    "            yield i, j\n",
    "\n",
    "\n",
    "def get_batch_of_paths(num_paths, chunk_limit=100):\n",
    "    \"\"\"Generate sequential batches of (i, j) path pairs.\"\"\"\n",
    "    pairs = pair_generator(num_paths) \n",
    "    chunk = [] \n",
    "\n",
    "    for idx, (i, j) in enumerate(pairs, 1):\n",
    "        chunk.append((i, j))\n",
    "\n",
    "        if idx % chunk_limit == 0:\n",
    "            yield chunk \n",
    "            chunk = [] \n",
    "\n",
    "    if chunk:  \n",
    "        yield chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from utils.features import WordUnit\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "def load_word(word_path, word_id, align_df):\n",
    "\n",
    "    \"\"\"Loads a word unit with metadata and encoding information.\"\"\"\n",
    "    # Load encoding units\n",
    "    units = np.load(word_path)\n",
    "    \n",
    "    # Extract filename and word index\n",
    "    parts = word_path.stem.split(\"_\")\n",
    "    filename, index = parts[0], int(parts[1])\n",
    "\n",
    "    # Filter align_df once using .query()\n",
    "    word_df = align_df.query(\"filename == @filename and word_id == @index\")\n",
    "    \n",
    "    if word_df.empty:\n",
    "        return None  # Early exit if word not found\n",
    "\n",
    "    # Extract the actual word text efficiently\n",
    "    true_word = word_df[\"text\"].iat[0] if isinstance(word_df[\"text\"].iat[0], str) else \"_\"\n",
    "\n",
    "    # Create WordUnit object\n",
    "    word = WordUnit(\n",
    "        id=word_id,\n",
    "        filename=filename,\n",
    "        index=index,\n",
    "        true_word=true_word,\n",
    "        boundaries=[word_df[\"word_start\"].iat[0], word_df[\"word_end\"].iat[0]],\n",
    "    )\n",
    "\n",
    "    # Update encoding with loaded units\n",
    "    word.update_encoding(units)\n",
    "    return word\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_key(key, file_map, words_cache, keys, align_df):\n",
    "    \"\"\"Helper function to process a single key.\"\"\"\n",
    "    if key in words_cache:\n",
    "        return words_cache[key]  # Retrieve from cache\n",
    "    \n",
    "    path = file_map.get(key)\n",
    "    if path is None:\n",
    "        print(f\"Warning: No file found for key '{key}' in file_map\")\n",
    "        return None  # Skip processing for missing files\n",
    "\n",
    "    word = load_word(path, key, align_df)  # Load word\n",
    "    words_cache[key] = word  # Cache it\n",
    "    keys.add(key)\n",
    "    return word\n",
    "\n",
    "def load_units_for_chunk(dataset, chunk, align_df=None, file_map=None):\n",
    "    \"\"\"Optimized function for loading units for a chunk with parallel loading using joblib.\"\"\"\n",
    "\n",
    "    # Use Parquet if available for faster reading\n",
    "    if align_df is None:\n",
    "        csv_path = dataset.align_dir / \"alignments.csv\"\n",
    "        align_df = pd.read_csv(csv_path)\n",
    "    \n",
    "    words_cache = {}  # Cache for fast word retrieval\n",
    "    keys = set()\n",
    "    chunk_words = []\n",
    "\n",
    "    # Process words in parallel using joblib\n",
    "    for pair in chunk:\n",
    "        pair_keys = tuple(pair.keys())\n",
    "\n",
    "        words= []\n",
    "        for key in pair_keys:\n",
    "            words.append(process_key(key, file_map, words_cache, keys, align_df))\n",
    "            \n",
    "        chunk_words.append(tuple(words))\n",
    "\n",
    "    return chunk_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import editdistance\n",
    "\n",
    "def calculate_distance_per_chunk(chunk_words):\n",
    "    \"\"\"Process sub-chunk and return computed distances with indices\"\"\"\n",
    "    results = []\n",
    "\n",
    "    for pair in chunk_words:\n",
    "        encoding_i = torch.from_numpy(pair[0].clean_encoding)\n",
    "        encoding_j = torch.from_numpy(pair[1].clean_encoding)\n",
    "\n",
    "        shapes = torch.tensor([encoding_i.size()[0], encoding_j.size()[0]])\n",
    "        length = torch.max(shapes)\n",
    "\n",
    "        dist = 0\n",
    "        if length > 0:\n",
    "            dist =  editdistance.eval(encoding_i, encoding_j) / length\n",
    "\n",
    "        results.append((pair[0].id, pair[1].id, dist))\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "num_features = len(sample_feature_paths)\n",
    "dist_mat = torch.zeros((sample_size, sample_size), dtype=torch.float32)\n",
    "align_df = pd.read_csv(dataset.align_dir / \"alignments.csv\")\n",
    "\n",
    "chunk_limit = 100\n",
    "num_pairs = num_features * (num_features - 1) // 2\n",
    "num_chunks = (num_pairs + chunk_limit - 1) // chunk_limit \n",
    "\n",
    "start_time = time.perf_counter()\n",
    "for chunk in tqdm(get_batch_of_paths(num_features, chunk_limit=100), total=num_chunks, desc=\"Processing chunks\"):\n",
    "    chunk_paths = [{i: sample_feature_paths[i], j: sample_feature_paths[j]} for i, j in chunk]\n",
    "\n",
    "    chunk_words = load_units_for_chunk(\n",
    "        dataset, chunk_paths, align_df=align_df, file_map=file_map\n",
    "    )\n",
    "    results = calculate_distance_per_chunk(chunk_words)\n",
    "    \n",
    "    for i,j, distance in results:\n",
    "        dist_mat[i, j] = distance \n",
    "\n",
    "end_time = time.perf_counter()\n",
    "\n",
    "print(f\"Total time: {end_time - start_time}s\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
