{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting units\n",
    "Set `get=True` if units need to be extracted for specified `gamma` and `layer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample size: 2703\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from encode import sample_files, get_units\n",
    "from pathlib import Path\n",
    "\n",
    "get = False\n",
    "align_dir = Path(\"data/alignments/dev-clean/\")\n",
    "align_path = align_dir / \"alignments.csv\"\n",
    "audio_dir = Path(\"data/dev-clean\")\n",
    "audio_ext = \".flac\"\n",
    "\n",
    "gamma = 0.5\n",
    "layer = 7\n",
    "save_dir = Path(\"features/\")\n",
    "\n",
    "align_df = pd.read_csv(align_path)\n",
    "\n",
    "paths, sample_size = sample_files(\n",
    "    audio_dir=audio_dir, audio_ext=audio_ext, sample_size=-1\n",
    ")\n",
    "\n",
    "print(f\"Sample size: {sample_size}\")\n",
    "if not Path(save_dir / str(gamma)).exists():\n",
    "    get_units(paths, align_df, audio_dir, gamma, layer, save_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate Distances \n",
    "Calculates pairwise distances for all the pairs in the dataset chunkwise. Set `chunk_limit` and `out_dir`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dist import get_features, get_batch_of_paths, cal_dist_per_pair\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "chunk_limit = 5000000\n",
    "out_dir = Path(f\"output/{gamma}/temp/\")\n",
    "out_dir.mkdir(parents=True, exist_ok=True)\n",
    "preloaded = True\n",
    "\n",
    "\n",
    "def process_batch(batch, features):\n",
    "    \"\"\"Parallelized function to calculate distance for each (i, j) pair.\"\"\"\n",
    "    return [cal_dist_per_pair(((i, j), (features[i], features[j]))) for i, j in batch]\n",
    "\n",
    "\n",
    "if not preloaded:\n",
    "    paths = (p for p in Path(f\"features/{gamma}\").rglob(\"**/*.npy\"))\n",
    "    sorted_paths = sorted(paths, key=lambda x: int(x.stem.split(\"_\")[-1]))\n",
    "    sample_size = len(sorted_paths)\n",
    "\n",
    "    features = get_features(sorted_paths)\n",
    "\n",
    "    rows, cols, vals = [], [], []\n",
    "\n",
    "    num_pairs = sample_size * (sample_size - 1) // 2\n",
    "    num_batches = (num_pairs + chunk_limit - 1) // chunk_limit\n",
    "\n",
    "    print(f\"num_samples: {sample_size}\")\n",
    "    print(f\"num_pairs: {num_pairs}\")\n",
    "\n",
    "    chunk_idx = 0\n",
    "    # Parallel execution\n",
    "    for batch in tqdm(\n",
    "        get_batch_of_paths(sample_size, chunk_limit),\n",
    "        total=num_batches,\n",
    "        unit=\"batch\",\n",
    "        mininterval=10.0,\n",
    "        desc=\"Processing Batches\",\n",
    "    ):\n",
    "        for i, j in batch:\n",
    "            i, j, dist = cal_dist_per_pair(((i, j), (features[i], features[j])))\n",
    "            rows.append(i)\n",
    "            cols.append(j)\n",
    "            vals.append(dist)\n",
    "\n",
    "        np.save(out_dir / f\"temp_rows_{chunk_idx}.npy\", rows)\n",
    "        np.save(out_dir / f\"temp_cols_{chunk_idx}.npy\", cols)\n",
    "        np.save(out_dir / f\"temp_vals_{chunk_idx}.npy\", vals)\n",
    "\n",
    "        rows, cols, vals = [], [], []\n",
    "        chunk_idx += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build graph from temp files\n",
    "These temp files are then used to build the graph chunkwise. If the graph has been computed before, it can only be read in. \n",
    "\n",
    "A search is performed to get the resolution `res` that gets the correct amount of clusters. The partition is stored in a .csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded precomputed graph from output/0.5/graph.pkl\n"
     ]
    }
   ],
   "source": [
    "from cluster import build_graph_from_temp\n",
    "import pickle\n",
    "\n",
    "use_preloaded_graph = True\n",
    "num_clusters = 13967\n",
    "temp_dir = Path(f\"output/{gamma}/temp\")\n",
    "temp_dir.mkdir(parents=True, exist_ok=True)  # Ensure the directory exists\n",
    "\n",
    "graph_path = Path(f\"output/{gamma}/graph.pkl\")\n",
    "\n",
    "if use_preloaded_graph and graph_path.exists():\n",
    "    with open(graph_path, \"rb\") as f:\n",
    "        g = pickle.load(f)\n",
    "    print(f\"Loaded precomputed graph from {graph_path}\")\n",
    "else:\n",
    "    g = build_graph_from_temp(temp_dir, 399)\n",
    "    g.write_pickle(str(graph_path))\n",
    "    print(f\"Graph built and saved to {graph_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And get the partition dataframe (which node belongs in which cluster):\n",
    "\n",
    "\n",
    "Set `use_predefined_partition=False` if the partition must be calculated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1: res=0.026500, Cluster difference=56\n",
      "Iteration 2: res=0.027500, Cluster difference=46\n",
      "Iteration 3: res=0.026550, Cluster difference=60\n",
      "Iteration 4: res=0.027453, Cluster difference=44\n",
      "Iteration 5: res=0.026595, Cluster difference=68\n",
      "Iteration 6: res=0.027410, Cluster difference=27\n",
      "Iteration 7: res=0.026636, Cluster difference=65\n",
      "Iteration 8: res=0.027371, Cluster difference=42\n",
      "Iteration 9: res=0.026673, Cluster difference=46\n",
      "Iteration 10: res=0.027336, Cluster difference=40\n",
      "Best resolution found: 0.027410 with cluster difference: 27\n"
     ]
    }
   ],
   "source": [
    "from cluster import adaptive_res_search\n",
    "\n",
    "use_predefined_partition = False\n",
    "\n",
    "partition_pattern = Path(f\"output/{gamma}\").glob(\"partition_r*.csv\")\n",
    "partition_files = list(partition_pattern)\n",
    "\n",
    "if not partition_files or not use_predefined_partition:\n",
    "    # No existing partitions found, run the search\n",
    "    best_res, best_partition = adaptive_res_search(\n",
    "        g, num_clusters, 0.0275, max_iters=10\n",
    "    )\n",
    "\n",
    "    # Convert best_partition to a DataFrame\n",
    "    best_partition_df = pd.DataFrame(\n",
    "        {\n",
    "            \"node\": range(len(best_partition.membership)),  # Node IDs\n",
    "            \"cluster\": best_partition.membership,  # Cluster assignments\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Save to CSV\n",
    "    best_partition_df.to_csv(\n",
    "        f\"output/{gamma}/partition_r{round(best_res, 6)}.csv\", index=False\n",
    "    )\n",
    "else:\n",
    "    # Load existing partitions\n",
    "    res_partitions = [\n",
    "        (float(p.stem.split(\"_r\")[1]), pd.read_csv(p)) for p in partition_files\n",
    "    ]\n",
    "\n",
    "    # Find the partition with the minimum resolution\n",
    "    best_res, best_partition_df = min(res_partitions, key=lambda x: x[0])\n",
    "\n",
    "# Ensure best_partition_df is used for further processing\n",
    "actual_clusters = len(set(best_partition_df[\"cluster\"]))\n",
    "diff = abs(actual_clusters - num_clusters)\n",
    "\n",
    "print(f\"Best resolution found: {best_res:.6f} with cluster difference: {diff}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate\n",
    "The graph partition is evaluated by computing NED for the text in each cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Appending Text and Phones:   2%|‚ñè         | 1513/63137 [00:05<03:41, 278.71it/s]"
     ]
    }
   ],
   "source": [
    "from eval import transcribe_clusters, ned, print_clusters, get_phones_and_texts\n",
    "\n",
    "phones, texts = get_phones_and_texts(gamma, align_dir)\n",
    "phone_clusters = transcribe_clusters(best_partition_df, phones, texts)\n",
    "ned_val, dist_p_cluster = ned(phone_clusters, num_clusters - diff)\n",
    "print(f\"NED: {ned_val}\")\n",
    "print_clusters(dist_p_cluster)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And to update the readme:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated README.md with gamma=0.5, res=0.025909631250000002, NED=0.1363889572950492\n"
     ]
    }
   ],
   "source": [
    "from eval import update_readme\n",
    "\n",
    "update_readme(gamma, best_res, ned_val, diff)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
