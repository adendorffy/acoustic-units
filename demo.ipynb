{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting units\n",
    "Set `get=True` if units need to be extracted for specified `gamma` and `layer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample size: 2703\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/danel/.cache/torch/hub/bshall_dusted_main\n",
      "Using cache found in /home/danel/.cache/torch/hub/bshall_dusted_main\n",
      "Using cache found in /home/danel/.cache/torch/hub/bshall_hubert_main\n",
      "Getting units: 100%|██████████| 2703/2703 [14:59<00:00,  3.01it/s]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from encode import sample_files, get_units\n",
    "from pathlib import Path\n",
    "\n",
    "get = False\n",
    "align_dir = Path(\"data/alignments/dev-clean/\")\n",
    "align_path = align_dir / \"alignments.csv\"\n",
    "audio_dir = Path(\"data/dev-clean\")\n",
    "audio_ext = \".flac\"\n",
    "\n",
    "gamma = 0.1\n",
    "layer = 7\n",
    "save_dir = Path(\"features/\")\n",
    "\n",
    "align_df = pd.read_csv(align_path)\n",
    "\n",
    "paths, sample_size = sample_files(\n",
    "    audio_dir=audio_dir, audio_ext=audio_ext, sample_size=-1\n",
    ")\n",
    "\n",
    "print(f\"Sample size: {sample_size}\")\n",
    "if not Path(save_dir / str(gamma)).exists():\n",
    "    get_units(paths, align_df, audio_dir, gamma, layer, save_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate Distances \n",
    "Calculates pairwise distances for all the pairs in the dataset chunkwise. Set `chunk_limit` and `out_dir`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Appending Features: 100%|██████████| 63137/63137 [00:04<00:00, 15443.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_samples: 63137\n",
      "num_pairs: 1993108816\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches: 100%|██████████| 399/399 [1:22:40<00:00, 12.43s/batch]\n"
     ]
    }
   ],
   "source": [
    "from dist import get_features, get_batch_of_paths, cal_dist_per_pair\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "chunk_limit = 5000000\n",
    "out_dir = Path(f\"output/{gamma}/temp/\")\n",
    "out_dir.mkdir(parents=True, exist_ok=True)\n",
    "preloaded = False\n",
    "\n",
    "\n",
    "def process_batch(batch, features):\n",
    "    \"\"\"Parallelized function to calculate distance for each (i, j) pair.\"\"\"\n",
    "    return [cal_dist_per_pair(((i, j), (features[i], features[j]))) for i, j in batch]\n",
    "\n",
    "\n",
    "if not preloaded:\n",
    "    paths = (p for p in Path(f\"features/{gamma}\").rglob(\"**/*.npy\"))\n",
    "    sorted_paths = sorted(paths, key=lambda x: int(x.stem.split(\"_\")[-1]))\n",
    "    sample_size = len(sorted_paths)\n",
    "\n",
    "    features = get_features(sorted_paths)\n",
    "\n",
    "    rows, cols, vals = [], [], []\n",
    "\n",
    "    num_pairs = sample_size * (sample_size - 1) // 2\n",
    "    num_batches = (num_pairs + chunk_limit - 1) // chunk_limit\n",
    "\n",
    "    print(f\"num_samples: {sample_size}\")\n",
    "    print(f\"num_pairs: {num_pairs}\")\n",
    "\n",
    "    chunk_idx = 0\n",
    "    # Parallel execution\n",
    "    for batch in tqdm(\n",
    "        get_batch_of_paths(sample_size, chunk_limit),\n",
    "        total=num_batches,\n",
    "        unit=\"batch\",\n",
    "        mininterval=10.0,\n",
    "        desc=\"Processing Batches\",\n",
    "    ):\n",
    "        for i, j in batch:\n",
    "            i, j, dist = cal_dist_per_pair(((i, j), (features[i], features[j])))\n",
    "            rows.append(i)\n",
    "            cols.append(j)\n",
    "            vals.append(dist)\n",
    "\n",
    "        np.save(out_dir / f\"temp_rows_{chunk_idx}.npy\", rows)\n",
    "        np.save(out_dir / f\"temp_cols_{chunk_idx}.npy\", cols)\n",
    "        np.save(out_dir / f\"temp_vals_{chunk_idx}.npy\", vals)\n",
    "\n",
    "        rows, cols, vals = [], [], []\n",
    "        chunk_idx += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build graph from temp files\n",
    "These temp files are then used to build the graph chunkwise. If the graph has been computed before, it can only be read in. \n",
    "\n",
    "A search is performed to get the resolution `res` that gets the correct amount of clusters. The partition is stored in a .csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded precomputed graph from output/0.1/graph.pkl\n",
      "Best resolution found: 0.027 with cluster difference: 11\n"
     ]
    }
   ],
   "source": [
    "from cluster import build_graph_from_temp, adaptive_res_search\n",
    "import pickle\n",
    "\n",
    "use_preloaded_graph = True\n",
    "num_clusters = 13967\n",
    "temp_dir = Path(f\"output/{gamma}/temp\")\n",
    "temp_dir.mkdir(parents=True, exist_ok=True)  # Ensure the directory exists\n",
    "\n",
    "graph_path = Path(f\"output/{gamma}/graph.pkl\")\n",
    "\n",
    "if use_preloaded_graph and graph_path.exists():\n",
    "    with open(graph_path, \"rb\") as f:\n",
    "        g = pickle.load(f)\n",
    "    print(f\"Loaded precomputed graph from {graph_path}\")\n",
    "else:\n",
    "    g = build_graph_from_temp(temp_dir, 399)\n",
    "    g.write_pickle(str(graph_path))\n",
    "    print(f\"Graph built and saved to {graph_path}\")\n",
    "\n",
    "partition_pattern = Path(f\"output/{gamma}\").glob(\"best_partition_r*.csv\")\n",
    "partition_files = list(partition_pattern)\n",
    "\n",
    "if not partition_files:\n",
    "    # No existing partitions found, run the search\n",
    "    best_res, best_partition = adaptive_res_search(g, num_clusters)\n",
    "\n",
    "    # Convert best_partition to a DataFrame\n",
    "    best_partition_df = pd.DataFrame(\n",
    "        {\n",
    "            \"node\": range(len(best_partition.membership)),  # Node IDs\n",
    "            \"cluster\": best_partition.membership,  # Cluster assignments\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Save to CSV\n",
    "    best_partition_df.to_csv(\n",
    "        f\"output/{gamma}/best_partition_r{round(best_res, 3)}.csv\", index=False\n",
    "    )\n",
    "else:\n",
    "    # Load existing partitions\n",
    "    res_partitions = [\n",
    "        (float(p.stem.split(\"_r\")[1]), pd.read_csv(p)) for p in partition_files\n",
    "    ]\n",
    "\n",
    "    # Find the partition with the minimum resolution\n",
    "    best_res, best_partition_df = min(res_partitions, key=lambda x: x[0])\n",
    "\n",
    "# Ensure best_partition_df is used for further processing\n",
    "actual_clusters = len(set(best_partition_df[\"cluster\"]))\n",
    "diff = abs(actual_clusters - num_clusters)\n",
    "\n",
    "print(f\"Best resolution found: {best_res:.3f} with cluster difference: {diff}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate\n",
    "The graph partition is evaluated by computing NED for the text in each cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded texts from features/0.1/texts.csv\n",
      "NED: 0.8234904471595739\n"
     ]
    }
   ],
   "source": [
    "from eval import (\n",
    "    get_phones_and_texts,\n",
    "    transcribe_clusters_into_phones,\n",
    "    transcribe_clusters,\n",
    "    ned,\n",
    ")\n",
    "\n",
    "phones, texts = get_phones_and_texts(gamma, align_dir)\n",
    "phone_clusters = transcribe_clusters_into_phones(best_partition_df, phones)\n",
    "text_clusters = transcribe_clusters(best_partition_df, texts)  # if you want to print it\n",
    "ned_val = ned(phone_clusters)\n",
    "\n",
    "print(f\"NED: {ned_val}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And to update the readme:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated README.md with gamma=0.1, res=0.027, NED=0.8234904471595739\n"
     ]
    }
   ],
   "source": [
    "from eval import update_readme\n",
    "\n",
    "update_readme(gamma, best_res, ned_val, diff)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
