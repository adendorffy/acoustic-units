{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting units\n",
    "Set `get=True` if units need to be extracted for specified `gamma` and `layer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample size: 2703\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from encode import sample_files, get_units\n",
    "from pathlib import Path\n",
    "\n",
    "get = False\n",
    "align_dir = Path(\"data/alignments/dev-clean/\")\n",
    "align_path = align_dir / \"alignments.csv\"\n",
    "audio_dir = Path(\"data/dev-clean\")\n",
    "audio_ext = \".flac\"\n",
    "\n",
    "gamma = 0.3\n",
    "layer = 7\n",
    "save_dir = Path(\"features/\")\n",
    "\n",
    "align_df = pd.read_csv(align_path)\n",
    "\n",
    "paths, sample_size = sample_files(\n",
    "    audio_dir=audio_dir, audio_ext=audio_ext, sample_size=-1\n",
    ")\n",
    "\n",
    "print(f\"Sample size: {sample_size}\")\n",
    "if get:\n",
    "    get_units(paths, align_df, audio_dir, gamma, layer, save_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate Distances \n",
    "Calculates pairwise distances for all the pairs in the dataset chunkwise. Set `chunk_limit` and `out_dir`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dist import get_features, get_batch_of_paths, cal_dist_per_pair\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "chunk_limit = 5000000\n",
    "out_dir = Path(f\"output/{gamma}/temp/\")\n",
    "out_dir.mkdir(parents=True, exist_ok=True)\n",
    "preloaded = True\n",
    "\n",
    "if not preloaded:\n",
    "    paths = (p for p in Path(f\"features/{gamma}\").rglob(\"**/*.npy\"))\n",
    "    sorted_paths = sorted(paths, key=lambda x: int(x.stem.split(\"_\")[-1]))\n",
    "    sample_size = len(sorted_paths)\n",
    "\n",
    "    features = get_features(sorted_paths)\n",
    "\n",
    "    rows, cols, vals = [], [], []\n",
    "\n",
    "    num_pairs = sample_size * (sample_size - 1) // 2\n",
    "    num_batches = (num_pairs + chunk_limit - 1) // chunk_limit\n",
    "\n",
    "    print(f\"num_samples: {sample_size}\")\n",
    "    print(f\"num_pairs: {num_pairs}\")\n",
    "\n",
    "    chunk_idx = 0\n",
    "    for batch in tqdm(\n",
    "        get_batch_of_paths(sample_size, chunk_limit),\n",
    "        total=num_batches,\n",
    "        unit=\"batch\",\n",
    "        mininterval=10.0,\n",
    "        desc=\"Processing Batches\",\n",
    "    ):\n",
    "        for i, j in batch:\n",
    "            i, j, dist = cal_dist_per_pair(((i, j), (features[i], features[j])))\n",
    "            rows.append(i)\n",
    "            cols.append(j)\n",
    "            vals.append(dist)\n",
    "\n",
    "        np.save(out_dir / f\"temp_rows_{chunk_idx}.npy\", rows)\n",
    "        np.save(out_dir / f\"temp_cols_{chunk_idx}.npy\", cols)\n",
    "        np.save(out_dir / f\"temp_vals_{chunk_idx}.npy\", vals)\n",
    "\n",
    "        rows, cols, vals = [], [], []\n",
    "        chunk_idx += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build graph from temp files\n",
    "These temp files are then used to build the graph chunkwise. If the graph has been computed before, it can only be read in. \n",
    "\n",
    "A search is performed to get the resolution `res` that gets the correct amount of clusters. The partition is stored in a .csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating total: 100%|██████████| 399/399 [00:30<00:00, 13.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_size: 1993108816, sample_size: 63137\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Getting Temp Info: 100%|██████████| 399/399 [03:01<00:00,  2.20it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "_write_graph_to_pickle_file() got an unexpected keyword argument 'format'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 17\u001b[39m\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     16\u001b[39m     g = build_graph_from_temp(temp_dir, \u001b[32m399\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m     \u001b[43mg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwrite_pickle\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mgraph_path\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpickle\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     18\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mGraph built and saved to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgraph_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     20\u001b[39m best_res, best_partition = adaptive_res_search(g, num_clusters)\n",
      "\u001b[31mTypeError\u001b[39m: _write_graph_to_pickle_file() got an unexpected keyword argument 'format'"
     ]
    }
   ],
   "source": [
    "from cluster import build_graph_from_temp, adaptive_res_search\n",
    "import pickle\n",
    "\n",
    "use_preloaded_graph = True\n",
    "num_clusters = 13967\n",
    "temp_dir = Path(f\"output/{gamma}/temp\")\n",
    "temp_dir.mkdir(parents=True, exist_ok=True)  # Ensure the directory exists\n",
    "\n",
    "graph_path = Path(f\"output/{gamma}/graph.pkl\")\n",
    "\n",
    "if use_preloaded_graph and graph_path.exists():\n",
    "    with open(graph_path, \"rb\") as f:\n",
    "        g = pickle.load(f)\n",
    "    print(f\"Loaded precomputed graph from {graph_path}\")\n",
    "else:\n",
    "    g = build_graph_from_temp(temp_dir, 399)\n",
    "    g.write_pickle(str(graph_path))\n",
    "    print(f\"Graph built and saved to {graph_path}\")\n",
    "\n",
    "best_res, best_partition = adaptive_res_search(g, num_clusters)\n",
    "\n",
    "actual_clusters = len(set(best_partition.membership))\n",
    "diff = abs(actual_clusters - num_clusters)\n",
    "\n",
    "print(f\"Best resolution found: {best_res:.3f} with cluster difference: {diff}\")\n",
    "\n",
    "df = pd.DataFrame(\n",
    "    {\n",
    "        \"node\": range(len(best_partition.membership)),  # Node IDs\n",
    "        \"cluster\": best_partition.membership,  # Cluster assignments\n",
    "    }\n",
    ")\n",
    "\n",
    "df.to_csv(f\"output/{gamma}/best_partition_r{round(best_res, 3)}.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate\n",
    "The graph partition is evaluated by computing NED for the text in each cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from eval import get_texts, transcribe_clusters, ned\n",
    "\n",
    "partition = pd.read_csv(f\"output/{gamma}/best_partition_r{round(best_res, 3)}.csv\")\n",
    "texts = get_texts(gamma, align_dir)\n",
    "\n",
    "cluster_transcriptions = transcribe_clusters(partition, texts)\n",
    "ned_val = ned(cluster_transcriptions)\n",
    "print(f\"NED: {ned_val}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And to update the readme:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from eval import update_readme\n",
    "\n",
    "update_readme(gamma, best_res, ned_val)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
