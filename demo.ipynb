{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting units\n",
    "Set `get=True` if units need to be extracted for specified `gamma` and `layer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample size: 2703\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from encode import sample_files, get_units\n",
    "from pathlib import Path\n",
    "\n",
    "get = False\n",
    "align_dir = Path(\"data/alignments/dev-clean/\")\n",
    "align_path = align_dir / \"alignments.csv\"\n",
    "audio_dir = Path(\"data/dev-clean\")\n",
    "audio_ext = \".flac\"\n",
    "\n",
    "gamma = 0.3\n",
    "layer = 7\n",
    "save_dir = Path(\"features/\")\n",
    "\n",
    "align_df = pd.read_csv(align_path)\n",
    "\n",
    "paths, sample_size = sample_files(\n",
    "    audio_dir=audio_dir, audio_ext=audio_ext, sample_size=-1\n",
    ")\n",
    "\n",
    "print(f\"Sample size: {sample_size}\")\n",
    "if get:\n",
    "    get_units(paths, align_df, audio_dir, gamma, layer, save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "align_path = Path(\"alignments.csv\")\n",
    "audio_dir = Path(\"data/dev-clean\")\n",
    "audio_ext = \".flac\"\n",
    "\n",
    "gamma = 0.3\n",
    "layer = 7\n",
    "save_dir = Path(\"features/\")\n",
    "\n",
    "align_df = pd.read_csv(align_path)\n",
    "\n",
    "paths, sample_size = sample_files(\n",
    "    audio_dir=audio_dir, audio_ext=audio_ext, sample_size=-1\n",
    ")\n",
    "\n",
    "print(f\"Sample size: {sample_size}\")\n",
    "if get:\n",
    "    get_units(paths, align_df, audio_dir, gamma, layer, save_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate Distances \n",
    "Calculates pairwise distances for all the pairs in the dataset chunkwise. Set `chunk_limit` and `out_dir`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dist import get_features, get_batch_of_paths, cal_dist_per_pair\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "chunk_limit = 5000000\n",
    "out_dir = Path(f\"output/{gamma}/temp/\")\n",
    "out_dir.mkdir(parents=True, exist_ok=True)\n",
    "preloaded = True\n",
    "\n",
    "if not preloaded:\n",
    "    paths = (p for p in Path(f\"features/{gamma}\").rglob(\"**/*.npy\"))\n",
    "    sorted_paths = sorted(paths, key=lambda x: int(x.stem.split(\"_\")[-1]))\n",
    "    sample_size = len(sorted_paths)\n",
    "\n",
    "    features = get_features(sorted_paths)\n",
    "\n",
    "    rows, cols, vals = [], [], []\n",
    "\n",
    "    num_pairs = sample_size * (sample_size - 1) // 2\n",
    "    num_batches = (num_pairs + chunk_limit - 1) // chunk_limit\n",
    "\n",
    "    print(f\"num_samples: {sample_size}\")\n",
    "    print(f\"num_pairs: {num_pairs}\")\n",
    "\n",
    "    chunk_idx = 0\n",
    "    for batch in tqdm(\n",
    "        get_batch_of_paths(sample_size, chunk_limit),\n",
    "        total=num_batches,\n",
    "        unit=\"batch\",\n",
    "        mininterval=10.0,\n",
    "        desc=\"Processing Batches\",\n",
    "    ):\n",
    "        for i, j in batch:\n",
    "            i, j, dist = cal_dist_per_pair(((i, j), (features[i], features[j])))\n",
    "            rows.append(i)\n",
    "            cols.append(j)\n",
    "            vals.append(dist)\n",
    "\n",
    "        np.save(out_dir / f\"temp_rows_{chunk_idx}.npy\", rows)\n",
    "        np.save(out_dir / f\"temp_cols_{chunk_idx}.npy\", cols)\n",
    "        np.save(out_dir / f\"temp_vals_{chunk_idx}.npy\", vals)\n",
    "\n",
    "        rows, cols, vals = [], [], []\n",
    "        chunk_idx += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build graph from temp files\n",
    "These temp files are then used to build the graph chunkwise. If the graph has been computed before, it can only be read in. \n",
    "\n",
    "A search is performed to get the resolution `res` that gets the correct amount of clusters. The partition is stored in a .csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cluster import build_graph_from_temp, adaptive_res_search\n",
    "import pickle\n",
    "\n",
    "use_preloaded_graph = True\n",
    "num_clusters = 13967\n",
    "temp_dir = Path(f\"output/{gamma}/temp\")\n",
    "temp_dir.mkdir(parents=True, exist_ok=True)  # Ensure the directory exists\n",
    "\n",
    "graph_path = Path(f\"output/{gamma}/graph.pkl\")\n",
    "\n",
    "if use_preloaded_graph and graph_path.exists():\n",
    "    with open(graph_path, \"rb\") as f:\n",
    "        g = pickle.load(f)\n",
    "    print(f\"Loaded precomputed graph from {graph_path}\")\n",
    "else:\n",
    "    g = build_graph_from_temp(temp_dir, 399)\n",
    "    g.write_pickle(str(graph_path), format=\"pickle\")\n",
    "    print(f\"Graph built and saved to {graph_path}\")\n",
    "\n",
    "best_res, best_partition = adaptive_res_search(g, num_clusters)\n",
    "\n",
    "actual_clusters = len(set(best_partition.membership))\n",
    "diff = abs(actual_clusters - num_clusters)\n",
    "\n",
    "print(f\"Best resolution found: {best_res:.3f} with cluster difference: {diff}\")\n",
    "\n",
    "df = pd.DataFrame(\n",
    "    {\n",
    "        \"node\": range(len(best_partition.membership)),  # Node IDs\n",
    "        \"cluster\": best_partition.membership,  # Cluster assignments\n",
    "    }\n",
    ")\n",
    "\n",
    "df.to_csv(f\"output/{gamma}/best_partition_r{round(best_res, 3)}.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate\n",
    "The graph partition is evaluated by computing NED for the text in each cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from eval import get_texts, transcribe_clusters, ned\n",
    "\n",
    "partition = pd.read_csv(f\"output/{gamma}/best_partition_r{round(best_res, 3)}.csv\")\n",
    "texts = get_texts(gamma, align_dir)\n",
    "\n",
    "cluster_transcriptions = transcribe_clusters(partition, texts)\n",
    "ned_val = ned(cluster_transcriptions)\n",
    "print(f\"NED: {ned_val}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
